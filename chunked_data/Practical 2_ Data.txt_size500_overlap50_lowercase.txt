tim for a binary search tree , the order of inserting values affects the shape of the tree . our goal with binary search trees is to minimize their height . the ideal binary search tree is complete , where each node above the last level has 2 subnodes , but that is hard to maintain . an avl tree is an approximately balanced binary search tree that maintains a balance factor value in each node to ensure balance . a node is considered imbalanced if the absolute value of the difference in the height of its left subtree and the height of its right subtree is greater than 1. a node that is imbalanced is considered the node of imbalance , and can be represented by alpha . there are four potential cases where an avl tree can become unbalanced . for left-left imbalance ( case 1 ) , the height of alpha ’ s left subtree is larger than the height of its right subtree , and the most recently added node was to the left of its parent . for left-right imbalance ( case 2 ) , the height of alpha ’ s left subtree is larger than the height of its right subtree , and the most recently added node was to the right of its parent . for right-left imbalance ( case 3 ) , the height of alpha ’ s right subtree is larger than the height of its left subtree , and the most recently added node was to the left of its parent . for right-right imbalance ( case 4 ) , the height of alpha ’ s right subtree is larger than the height of its left subtree , and the most recently added node was to the right of its parent . cases 1 and 4 and cases 2 and 3 are mirrors of each other . for case 1 and case 2 , we call alpha ’ s left child node a , while for case 3 and 4 , we call alpha ’ s right child node a. for case 2 , we call a ’ s right child node b , and for case 3 , we call a ’ s left child node b. to rebalance case 1 , we perform a single rotation by setting a ’ s right child to be alpha and moving a ’ s old right subtree to be the left subtree of alpha to rebalance case 4 , we perform a single rotation by setting a ’ s left child to be alpha and moving a ’ s old left subtree to be the right subtree of alpha to rebalance case 2 , we first perform a single rotation with nodes a and b by setting alpha ’ s new left child to be node b , setting node b ’ s new left child to be node a , and by setting a ’ s new right child to
case 2 , we first perform a single rotation with nodes a and b by setting alpha ’ s new left child to be node b , setting node b ’ s new left child to be node a , and by setting a ’ s new right child to be node b ’ s old left subtree . once we perform the first rotation for case 2 , the tree becomes case 1 , where the old node b can be considered node a , while alpha remains alpha ; the tree can be balanced using the same rotation needed to case 1. to rebalance case 3 , we first perform a single rotation with nodes a and b by setting alpha ’ s new right child to be node b , setting node b ’ s new right child to be node a , and by setting a ’ s new left child to be node b ’ s old right subtree . once we perform the first rotation for case 3 , the tree becomes case 4 , where the old node b can be considered node a , while alpha remains alpha ; the tree can be balanced using the same rotation needed to case 4. when you insert a node into an avl tree , you should update the heights on the path from the newly inserted node to the root node of the tree , checking for imbalance as you go . computer memory can be stored in a number of different locations ; in descending order of speed , they are the cpu , registers , the l1 cache , the l2 cache , ram , and sdd/hdd . sdd/hdd is multiple orders of magnitude slower than ram , but have lots of storage and are persistent . the goal of database systems is to minimize the number of sdd/hdd accesses since a worst case binary search search is significantly faster than a single additional access of sdd/hdd data . raw binary search trees are not good for this because a given node of the binary tree probably occupies only a fraction of any cache line . b-trees are a way to get better locality by putting multiple elements into each tree node . b-trees were originally invented for storing data structures on disk , where locality is even more crucial than with memory . accessing a disk location takes about 5ms = 5,000,000ns . therefore , if you are storing a tree on disk , you want to make sure that a given disk read is as effective as possible . b-trees have a high branching factor , much larger than 2 , which ensures that few disk reads are needed to navigate to the place where data is stored . b-trees may also be useful for in-memory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when b-trees
, which ensures that few disk reads are needed to navigate to the place where data is stored . b-trees may also be useful for in-memory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when b-trees were first introduced ! a b-tree of order m is a search tree in which each non-leaf node has up to m children . the actual elements of the collection are stored in the leaves of the tree , and the non-leaf nodes contain only keys . each leaf stores some number of elements ; the maximum number may be greater or ( typically ) less than m. the data structure satisfies several requirements : first , every path from the root to a leaf has the same length . second , if a node has n children , it contains n−1 keys . third , every node ( except the root ) is at least half full . fourth , the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer . ( this generalizes the bst invariant . ) fifth , the root has at least two children if it is not a leaf . because the height of the tree is uniformly the same and every node is at least half full , we are guaranteed that the asymptotic performance is o ( log n ) where n is the size of the collection . the real win is in the constant factors , of course . we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits . for example , if we are accessing a large disk database then our `` cache lines '' are memory blocks of the size that are read from disk . lookup in a b-tree is straightforward . given a node to start from , we use a simple linear or binary search to find whether the desired element is in the node , or if not , which child pointer to follow from the current node . insertion and deletion from a b-tree are more complicated . for insertion , we first find the appropriate leaf node into which the inserted element falls ( assuming it is not already in the tree ) . if there is already room in the node , the new element can be inserted simply . otherwise the current leaf is already full and must be split into two leaves , one of which acquires the new element . the parent is then updated to contain a new key and child pointer . if the parent is already full , the process ripples upwards , eventually possibly reaching the root . if the
and must be split into two leaves , one of which acquires the new element . the parent is then updated to contain a new key and child pointer . if the parent is already full , the process ripples upwards , eventually possibly reaching the root . if the root is split into two , then a new root is created with just two children , increasing the height of the tree by one . deletion works in the opposite way : the element is removed from the leaf . if the leaf becomes empty , a key is removed from the parent node . if that node is no longer at least half full , the keys of the parent node and its immediate right ( or left ) sibling are reapportioned among them so that invariant 3 is satisfied . if this is not possible , the parent node can be combined with that sibling , removing a key another level up in the tree and possibly causing a ripple all the way to the root . if the root has just two children , and they are combined , then the root is deleted and the new combined node becomes the root of the tree , reducing the height of the tree by one . hash tables effectively function like a python dictionary . the hash table itself is a 1-dimensional array . each index stores a list , which contains the key value pairs as individual tuples . the size of a hash table refers to how large the array is . the size of any hash table is denoted by the letter m. the load factor of a hash table is the number of inserted elements divided by the size of the hash table . the load factor of any hash table is denoted by the greek letter lambda . inserting into a hash table works by passing the key of a key-value pair into a hashing function . the inner mechanism of the hash function is not very important , but it typically returns the value mod the table size , so that the output is a valid index in the table . the key-value pair is then appended as a tuple to the end of the list stored at the index outputted by the hash function . looking up a key in a hash table works by passing the desired key through the hash function , accessing all the tuples in the list stored at the corresponding index of the hash table , and performing a linear search to match the searched-for key with the key in each key-value pair . we want to maintain a low load factor such that lambda is not greater than 0.9. if lambda becomes larger than 0.9 , we should rehash the table , which means using a larger hash table ( with more indices ) , updating the hash function accordingly , and passing all
want to maintain a low load factor such that lambda is not greater than 0.9. if lambda becomes larger than 0.9 , we should rehash the table , which means using a larger hash table ( with more indices ) , updating the hash function accordingly , and passing all of the stored values through the new hash function and re-storing them accordingly . the relational data model has several benefits . first , it is for the most part the standard data model and query language used in industry . second , the model is acid compliant , meaning that it adheres to the principles of atomicity , consistency , isolation , and durability . third , the relational model works well with highly structured data . fourth , the relational model can handle large amounts of data . finally , the relational model is well understood with lots of tooling and lots of experience . a transaction is a series of crud operations performed as a single unit of work . crud stands for create , read , update , delete , and refer to the main operations you can perform on a database . a transaction could be as simple as a select statement , or more involved like joint update statements , etc . an entire transaction either succeeds ( commit ) or fails ( rollback/abort ) . the four acid properties are atomicity , consistency , isolation , and durability . atomicity means that every transaction is treated as one unit ; the unit is either executed in full or no parts of it are executed . consistency means that any transaction takes the database from one consistent state to another . a consistent state is when all the data meets integrity conditions . isolation means that any two transactions can not affect each other if executed at the same time . an issue with isolation could only happen if one transaction is reading from the data that another transaction is writing . a dirty read is when the first transaction is able to read a row that has been written by the second transaction , but has not yet been committed . a non-repeatable read is when a transaction returns different data than what existed when it started executing . finally , phantom reads occur when a transaction reads data twice and inserts both instances instead of just once . durability means that once a transaction is completed and committed , the changes are permanent . shrek 5 is set to hit theaters on 12/23/2026 . to increase the capacity of a system , you can scale either vertically or horizontally . scaling vertically refers to adding more compute within one system . scaling horizontally means adding more systems with the same compute . a distributed system is a collection of independent computers that appear to its users as one computer . the main characteristics of distributed systems is that they operate concurrently ,
refers to adding more compute within one system . scaling horizontally means adding more systems with the same compute . a distributed system is a collection of independent computers that appear to its users as one computer . the main characteristics of distributed systems is that they operate concurrently , fail independently , and do not have a global clock . it is inevitable that distributed systems will need to have network partitioning , which means that the systems need to be partition tolerant . in other words , if something happens to one node , the system as a whole will be fine . the cap theorem has three parts : consistency , availability , and partition tolerance . the theorem states that you can always have two of the three , but never all three . consistency means that you will always get the same result from the system . availability means that you can always access the system . partition tolerance means that the system will continue to operate despite network issues . examples of products that include consistency and availability but not partition tolerance are relational database models like postgresql and mysql . examples of products that include consistency and partition tolerance but not availability are mongo , redis , and hbase . examples of products that include availability and partition tolerance but not consistency are couchdb , cassandra , and dynamodb . nidhi foundations & searching searching is the most common operation performed by a database system . in sql , the select statement is arguably the most versatile or complex because you can nest the statement . the baseline for efficiency is linear search : start at the beginning of a list and proceed element by element until you find what you ’ re looking for or you get to the last element and have not found it . if there are n elements , worst case scenario is going through all n values . a record is a collection of values for attributes of a single entity instance ( a row of a table ) . a collection is a set of records of the same entity type ( a table ) . some collections are stored sequentially like a list . search key is a value for an attribute from the entity type , could be more than 1 attribute . if each record takes up x bytes of memory , then for n records , we need n * x bytes of memory . there are 2 ways of storing the n * x bytes in ram memory : contiguously allocated list where all n * x bytes are in a single chunk of memory , and linked list , where there are x + 2 memory addresses ( that identify front versus back ) * n. for a linked list , each record needs x bytes + additional space for 1 or 2 memory addresses and individual records are linked
single chunk of memory , and linked list , where there are x + 2 memory addresses ( that identify front versus back ) * n. for a linked list , each record needs x bytes + additional space for 1 or 2 memory addresses and individual records are linked together in a type of chain using memory addresses . python technically does not have a true equivalent to a contiguous array . arrays are fast for random access but slow for random insertions ( anywhere but the end ) . linked lists are slow for random access but fast for random insertions . binary search keeps going to the middle iteratively/recursively , but it must be a sorted array . the input is an array of values in sorted order , target value . output is the location ( index ) of where target is located or some value indicating target was not found . the maximum number of searches = log base 2 of ( n ) . binary search only really applies to contiguously allocated list because it is super inefficient for linked lists . best case is the target is found at middle ; 1 comparison ( inside the loop ) . worst case is the target is not in the array ; log2 n comparisons , or o ( log2n ) time complexity . for linear search , best case is the target is found at the first element ; only 1 comparison , and worst case is the target is not in the array ; n comparisons ( o ( n ) time complexity ) . let ’ s say you have a table with two columns : id and specialval . assume data is stored on disk by the id column ’ s value . searching for a specific id would be fast , but if we want to search for a specific specialval , the only option is linear scan of that column . we can not store data on disk sorted by both id and specialval at the same time , because the data would have to be duplicated which is space inefficient . therefore , we need an external data structure to support faster searching by specialval than a linear scan . one of your current options is an array of tuples ( specialval , rownumber ) sorted by specialval , and we could use binary search to quickly locate a particular specialval and find its corresponding row in the table . but every insert into the table would be like inserting into a sorted array , which is slow . another option is a linked list of tuples ( specialval , rownumber ) sorted by specialval , but searching for a specialval would be slow , since a linear scan required . but inserting into the table would theoretically be quick to also add to the list . something with fast insert and fast search that would solve this
specialval , rownumber ) sorted by specialval , but searching for a specialval would be slow , since a linear scan required . but inserting into the table would theoretically be quick to also add to the list . something with fast insert and fast search that would solve this issue is a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent . extra notes about binary search and avl trees : https : //www.cs.rochester.edu/u/gildea/csc282/slides/c12-bst.pdf ttps : //ics.uci.edu/~thornton/ics46/notes/avltrees/ creating/inserting into a binary search tree conceptually let ’ s say you want to insert : 23 17 20 42 31 50 . 23 is the top node ( root ) , 17 will descend to the left because it is smaller , 20 will descend to the right of 17 , 43 will descend to the left from 23 , 31 will go left , 50 will go right from 43. the level order traversal of this tree would be 23 17 43 20 31 50. tree traversal types are preorder , post order , in order , and level order . how ? when processing 23 ( root ) , print 23 and then temporarily store 17 and 43 in an external data structure . then , print 17 and remove it from the external data structure and temporarily store 20. then , remove 43 from the external data and print , and temporarily store 31 and 50. then remove 20 and print , no child so prints 31 then print 50. temporarily storing data happens in a queue , and python has a special version called a deque . binary search tree in python : class binary tree node ( self , value , left , right ) value - integer left – binary tree node right – binary tree node root = binarytreenode ( 23 ) root.left = binarytreenode ( 17 ) root.right= binarytreenode ( 43 ) how to get 20 ? : root.left.right = binarytreenode ( 20 ) the order of values inserted into binary search tree matters ( changes the shape of the tree ) . for example , if the values are sorted , it would be an unbalanced tree . the goal is to minimize height of tree . minimum height is always going to be from a complete tree ( all nodes filled except for last level ) . an avl tree is an approximately balanced binary search tree , it is self balancing and maintains a balance factor in each node ( at that node , how balanced is the tree if that node were the root ) . a tree is an avl tree if the absolute value of the height of left sub tree minus the height of right sub tree is less than or equal to 1. basically height of left sub tree and right sub tree can not differ by
were the root ) . a tree is an avl tree if the absolute value of the height of left sub tree minus the height of right sub tree is less than or equal to 1. basically height of left sub tree and right sub tree can not differ by more than 1. inserting into an avl tree can balance or imbalance the tree , but it would only change on the path from the most recently inserted node to the root . once a tree becomes imbalanced , there is an algorithm that will rebalance the tree . alpha ( root where it is imbalanced ) node gets reorganized . inserting can cause an imbalance in 4 ways : * left left insertion : when you insert to the left subtree of the left child of the node of imbalance * left right case : when you insert into the right subtree of the left child of the node of imbalance * right left case : insert into the left subtree of the right child * right right : insert into the right subtree of the right child rebalancing : * rebalancing case 1 : single rotation ( also applies to case 4 ) , reassign c ’ s left pointer to point to t2 and reassign a ’ s right pointer to point to c * rebalancing case 2 : double rotation ( also applies to case 3 ) . need to separate t2 into its 2 children and basically do a single rotation twice how the reassignment of pointers happens : * case 4 ( rr ) : * a.right points to the left child of c * c.left points to a calculating height of a node is the maximum of the 2 heights of a node 's subtrees , then you add one to the larger height . memory : a cpu has 16 or 32 registers , depending on the processor . registers are the closest to the cpu , making them very fast but small and expensive . the l1 cache is larger than registers but slower and cheaper per byte . the l2 cache is even larger than l1 . ram is the primary memory and operates at nanosecond speed . secondary storage , such as ssds and hdds , is much slower , measured in milliseconds . ssds and hdds provide a lot of storage and are persistent , meaning they can survive power cycles , but they are extremely slow . to improve speed , database systems should minimize access to secondary storage . a 64-bit integer takes up 8 bytes of memory . let ’ s say there is an avl tree where each node has a key and a value , and then a left and right pointers . on a 64 bit machine , this would take 4 * 8 = 32 bytes ( assuming an integer takes 8 bytes ) but if each node is stored in
avl tree where each node has a key and a value , and then a left and right pointers . on a 64 bit machine , this would take 4 * 8 = 32 bytes ( assuming an integer takes 8 bytes ) but if each node is stored in a different block , each node would need 2048 bytes . for an avl tree , 1.44 log base 2 of n , where n is the number of nodes in the tree , is worst case . let ’ s say you have a sorted array of 128 integers . worst case binary search on 128 integers is way faster than a single additional disk access . even faster would be to do 3 children per node ( shallower/less tall tree ) . this would minimize secondary storage disk accesses , which is the most important thing for database systems . b+ tree b+ tree is designed to maximize the number of values stored in each disk block . each node has the maximum possible number of keys to reduce disk access . a node with n-1 keys has n children . the b+ tree is optimized for disk-based indexing by minimizing disk access . it is an m-way tree with order m , where m is the maximum number of keys in a node , and m+1 is the maximum number of children . all nodes , except the root , must be at least half full , but the root does not have this requirement . insertions always happen at the leaf level , and leaves are stored as a doubly linked list . keys in nodes are kept sorted . a b+ tree is shallower and wider than a bst or avl tree with the same number of nodes . there are two types of nodes : internal nodes , which store keys and pointers to children , and leaf nodes , which store keys and data . unlike b-trees , which are used for in-memory indexing , b+ trees are designed for disk-based indexing . a relational database management system ( rdbms ) increases efficiency through indexing , direct storage control , column-oriented or row-oriented storage ( with column-oriented being faster for some large data processing ) , materialized views , query optimization , caching , prefetching , precompiled stored procedures , and data replication and partitioning to improve performance and data management . column vs row-oriented storage column-oriented storage and row-oriented storage have different advantages depending on the use case . transactions must be fully completed or not executed at all , following a commit or rollback/abort process . this ensures data integrity , error recovery , concurrency control , reliable data storage , and simplified error handling . the relational model offers several benefits , including a mostly standard data model and query language , acid compliance ( atomicity , consistency , isolation , durability ) , strong support for highly structured data
error recovery , concurrency control , reliable data storage , and simplified error handling . the relational model offers several benefits , including a mostly standard data model and query language , acid compliance ( atomicity , consistency , isolation , durability ) , strong support for highly structured data , the ability to handle large amounts of data , and widespread understanding with extensive tooling and experience . acid : acid ensures reliable database transactions through four key properties : atomicity , meaning a transaction must be fully completed or not executed at all ( all or none ) ; consistency , transaction is always in a consistent state ( all data meets integrity constraints ) ; isolation , preventing one transaction from negatively affecting another , often managed through locking to avoid issues like dirty reads ( where a transaction reads uncommitted changes from another ) , non-repeatable reads ( where repeated queries within a transaction return different results due to committed changes by another transaction ) , and phantom reads ( where rows are added or deleted by another transaction while the first is still running ) ; and durability , guaranteeing that once a transaction is committed , its changes are permanent even in the event of a system failure . relational databases have downsides , such as schemas changing over time , not all apps needing full acid compliance , joins being expensive , and a lot of data being semi-structured or unstructured like json or xml . horizontal scaling can be hard , and some apps need something more performant , like real-time or low-latency systems . scaling : conventional wisdom is to scale vertically ( up , with bigger , more powerful systems ) until the demands of high-availability make it necessary to scale out with some type of distributed computing model . why ? because scaling up is easier since there is no need to really modify your architecture . but , there are practical and financial limits to this . however , there are modern systems that make horizontal scaling less problematic . one such solution is distributed systems . a distributed system is “ a collection of independent computers that appear to its users as one computer. ” -andrew tennenbaum characteristics of distributed systems are that computers operate concurrently , computers fail independently , no shared global clock . distributed storage has 2 directions : replication and sharding . replication is having the same data in multiple places . sharding is having the data broken up into groups . data is stored on more than 1 node , typically replicated ( each block of data is available on n nodes ) . distributed databases can be relational or non-relational . mysql and postgresql support replication and sharding . cockroachdb is a new player on the scene . many nosql systems support one or both models . network partitioning is inevitable ( network failures , system failures ) , so overall
) . distributed databases can be relational or non-relational . mysql and postgresql support replication and sharding . cockroachdb is a new player on the scene . many nosql systems support one or both models . network partitioning is inevitable ( network failures , system failures ) , so overall system needs to be partition tolerant , in other words , the system can keep running even with network partition . the cap theorem states it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees : consistency , availability , and partition tolerance . consistency means that every read receives the most recent write or error thrown . availability means that every request receives a ( non-error ) response - but no guarantee that the response contains the most recent write . partition tolerance means that the system can continue to operate despite arbitrary network issues . cap theorem applied to a database ( think of it like a 3 way venn diagram ) . consistency means that every user of the db has an identical view of the data at any given instant ( the definition of consistency in cap is different from that of acid . ) availability means that in the event of a failure , the database remains operational . partition tolerance means the database can maintain operations in the event of the network ’ s failing between two segments of the distributed system . consistency and availability means the system always responds with the latest data and every request gets a response , but may not be able to deal with network issues . consistency and partition tolerance means if system responds with data from a distributed store , it is always the latest , else data request is dropped . availability and partition tolerance means the system always sends are responds based on distributed store but may not be the absolute latest data . in reality , cap theorem says if you can not limit the number of faults , requests can be directed to any server , and you insist on serving every request , then you can not possibly be consistent . but it is interpreted as you must always give up something : consistency , availability , or tolerance to failure . nosql and key value databases acid transactions focus on “ data safety ” and are considered a pessimistic concurrency model because it assumes one transaction must protect itself from other transactions . aka it assumes that if something can go wrong , it will . conflicts are prevented by locking resources until a transaction is complete ( there are both read and write locks ) . the write lock analogy says that it ’ s like borrowing a book from a library , if you have it , no one else can . optimistic concurrency says that transactions do not obtain locks on data when they read or write
both read and write locks ) . the write lock analogy says that it ’ s like borrowing a book from a library , if you have it , no one else can . optimistic concurrency says that transactions do not obtain locks on data when they read or write . it is considered optimistic because it assumes conflicts are unlikely to occur . even if there is a conflict , everything will still be ok because you add last update timestamp and version number columns to every table and read them when changing . then , check at the end of transaction to see if any other transaction has caused them to be modified . low conflict systems ( backups , analytical databases , etc . ) are read heavy systems . the conflicts that arise can be handled by rolling back and re-running a transaction that notices a conflict , so , optimistic concurrency works well - allows for higher concurrency . high conflict systems work by rolling back and rerunning transactions that encounter a conflict , which is less efficient , so , a locking scheme ( pessimistic model ) might be preferable . nosql “ nosql ” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql . more common modern meaning is “ not only sql ” . but , sometimes thought of as non-relational databases . idea originally developed , in part , as a response to processing unstructured web-based data . acid alternative for distributed systems is called base . basically available guarantees the availability of the data ( per cap ) , but response can be “ failure ” / “ unreliable ” because the data is in an inconsistent or changing state ( system appears to work most of the time ) . soft state says the state of the system could change over time , even without input . changes could be result of eventual consistency . data stores don ’ t have to be write-consistent . replicas don ’ t have to be mutually consistent . eventual consistency means the system will eventually become consistent , and all writes will eventually stop so all nodes/replicas can be updated . key value databases key = value key-value stores are designed around simplicity , speed , and scalability . simplicity means the data model is extremely simple , comparatively , tables in a rdbms are very complex . simplicity lends itself to simple crud ops and api creation . speed means it is usually deployed as in-memory db ; retrieving a value given its key is typically a o ( 1 ) op because hash tables or similar data structs used under the hood . there is no concept of complex queries or joins because they slow things down . scalability means horizontal scaling is simple - add more nodes . typically concerned with eventual consistency , meaning in a distributed environment ,
because hash tables or similar data structs used under the hood . there is no concept of complex queries or joins because they slow things down . scalability means horizontal scaling is simple - add more nodes . typically concerned with eventual consistency , meaning in a distributed environment , the only guarantee is that all nodes will eventually converge on the same value . data science use cases include storing intermediate results from data preprocessing and exploratory data analysis ( eda ) or experiment/testing ( a/b ) results without using the production database , using a feature store for fast retrieval of frequently accessed features for model training and prediction , and model monitoring to store key performance metrics , especially for real-time inference . software engineering use cases include storing session information , where all session data can be saved with a single put or post and retrieved quickly with a get , user profiles and preferences , where user settings like language , time zone , and ui preferences can be fetched with a single get , shopping cart data , which must be tied to the user and accessible across browsers , machines , and sessions , and using a caching layer in front of a disk-based database for faster access . connection pooling does a bunch of stuff , basically makes it more efficient . redis : redis , or remote directory server , is an open-source , in-memory database that supports durability by saving snapshots to disk at intervals or using an append-only file to track changes for recovery after failures . it is sometimes called a data structure store and is primarily a key-value store , though it also supports models like graph , spatial , full-text search , vector , and time series . originally developed in 2009 in c++ , redis is extremely fast , handling over 100,000 set operations per second , and offers a rich collection of commands . however , it does not handle complex data , lacks secondary indexes , and only supports lookups by key . in redis , keys are typically strings but can be any binary sequence , while values can be various data types , including strings , lists , hashes , sets , sorted sets , and geospatial data . redis provides 16 default databases , numbered 0 to 15 , and is accessed through commands for setting and retrieving key-value pairs , with many language libraries available . the foundation data type is a string , which maps one string to another and is commonly used for caching frequently accessed html/css/js , storing config settings , user info , token management , counting views , or rate limiting . the hash type stores key-value entries as field-value pairs and is useful for representing objects , session management , user/event tracking , and active session tracking . the list type is a linked list of string values , commonly used for stacks
, counting views , or rate limiting . the hash type stores key-value entries as field-value pairs and is useful for representing objects , session management , user/event tracking , and active session tracking . the list type is a linked list of string values , commonly used for stacks and queues , queue management , logging , social media feeds , chat message history , and batch processing . linked lists allow efficient o ( 1 ) insertion at the front or end . the json type supports full json syntax and is stored in a binary tree-structure for fast access . the set type is an unordered collection of unique strings , useful for tracking unique items ( e.g. , ip addresses ) , primitive relations ( e.g. , students in a course ) , access control lists , and social network connections . redis-py is the standard client for python , maintained by redis itself why redis over mysql/s3 or other relational database ? because all data is stored in disk , so it is faster . latency issues ( much faster than select statements ) . redis pipelines help avoid multiple related calls to the server , which needs less network overhead . document databases and mongodb document database is a non-relational database that stores data as structured documents , usually in json . they are designed to be simple , flexible , and scalable . json ( javascript object notation ) is a lightweight data-interchange format , easy for humans to read and write , easy for machines to parse and generate . json is built on two structures . a collection of name/value pairs ( in various languages , this is operationalized as an object , record , struct , dictionary , hash table , keyed list , or associative array ) and an ordered list of values ( in most languages , this is operationalized as an array , vector , list , or sequence . ) these are two universal data structures supported by virtually all modern programming languages , which makes json a great data interchange format . bson is binary json , it is a binary-encoded serialization of a json-like document structure . it supports extended types not part of basic json ( e.g . date , binarydata , etc ) . it is lightweight and keeps space overhead to a minimum . it is traversable , which means it ’ s designed to be easily traversed , which is vitally important to a document db . it is efficient because encoding and decoding must be efficient . it is also supported by many modern programming languages . xml , or extensible markup language , is the precursor to json as data exchange format . xml and css are used together for web pages content and formatting . xml is structurally like html , but tag set is extensible . some tools/technologies used with xml are xpath ( a syntax
or extensible markup language , is the precursor to json as data exchange format . xml and css are used together for web pages content and formatting . xml is structurally like html , but tag set is extensible . some tools/technologies used with xml are xpath ( a syntax for retrieving specific elements from an xml doc ) , xquery ( a query language for interrogating xml documents ; the sql of xml ) , dtd ( document type definition - a language for describing the allowed structure of an xml document ) , and xslt ( extensible stylesheet language transformation – a tool to transform xml into other formats , including non-xml formats such as html . ) why document databases ? they address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data . oo programming : inheritance and composition of types . how do we save a complex object to a relational database ? we basically have to deconstruct it . the structure of a document is self-describing . they are well-aligned with apps that use json/xml as a transport layer mongodb mongodb started in 2007 after doubleclick was acquired by google , and 3 of its veterans realized the limitations of relational databases for serving over 400,000 ads per second . mongodb was short for humongous database . mongodb atlas released in 2016 , and provided documentdb as a service . no predefined schema for documents is needed . every document in a collection could have different data/schema . rich query support provides robust support for all crud ops . indexing supports primary and secondary indices on document fields . replication supports replica sets with automatic failover , and load balancing is built in . mongodb atlas is a fully managed mongodb service in the cloud ( dbaas ) . mongodb enterprise is a subscription-based , self-managed version of mongodb . mongodb community is source-available , free-to-use , self-managed . relational database versus mongo : a database in a relational database is called a database in mongodb . a table or view in a relational database is called a collection in mongodb . a row in a relational database is called a document in mongodb . a column in a relational database is called a field in mongodb . an index in a relational database is called an index in mongodb . a join in a relational database is called an embedded document in mongodb . a foreign key in a relational database is called a reference in mongodb . interacting with mongodb is done using the following . mongosh ( mongodb shell ) is a cli tool for interacting with a mongodb instance . mongodb compass is a free , open-source gui to work with a mongodb database . datagrip and other 3rd party tools can also be used . every major language has a library to interface with mongodb , such as pymongo ( python ) ,
interacting with a mongodb instance . mongodb compass is a free , open-source gui to work with a mongodb database . datagrip and other 3rd party tools can also be used . every major language has a library to interface with mongodb , such as pymongo ( python ) , mongoose ( javascript/node ) , and more . pymongo is a python library for interfacing with mongodb instances . intro to graph data model graph database is made up of nodes connected by edges , which can be directed or undirected . each edge and node is uniquely identifiable . you can also add extra properties via key value pairs . queries are based on the structure of database , called a cipher . examples of graph data model are social networks , web pages , chemical and biological data . the underlying thing that links pages of the internet ( which is a is big graph ) is called https . ( 404 error message ) a labelled property graph has edges/arcs/relationships and nodes/vertices . this is the type that has properties with keys and values . labels are used to mark a node as part of a group . nodes with no relationships are allowed but edges not connected to nodes are not . a path is getting from one node to another in order , with no repeating nodes or edges . it does not matter if you count nodes or edges in a graph algorithm as long as it is consistent . there are connected ( vs. disconnected ) graphs , connected means there is a path between any two nodes in the graph . weighted ( vs. unweighted ) means the edge has a weight property ( important for some algorithms ) . directed ( vs. undirected ) means relationships ( edges ) define a start and end node . directed graphs can be a loop with arrows going both ways . acyclic ( vs. cyclic ) means the graph contains no cycles . spare vs dense has to do with the number of edges compared to number of nodes . trees have no cycles . pathfinding has to do with finding the shortest path , which can mean fewest edges or lowest weight when summed . average shortest path can be used to monitor efficiency and resiliency of a network . pagerank search engines used to use the number of links to a website as the first search result , then it became how many pages link to the pages that link to the most page . breadth first search ( visit nearest neighbors first ) is different from depth first search ( walks down each branch first ) . centrality is determining which nodes are “ more important ” in a network compared to other nodes ( degree , closeness , betweenness , pagerank ) . community detection is evaluating clustering or partitioning of nodes of a graph and tendency to strengthen or
down each branch first ) . centrality is determining which nodes are “ more important ” in a network compared to other nodes ( degree , closeness , betweenness , pagerank ) . community detection is evaluating clustering or partitioning of nodes of a graph and tendency to strengthen or break apart . famous graph algorithms include dijkstra ’ s algorithm ( single-source shortest path algo for positively weighted graphs ) , a * algorithm ( similar to dijkstra ’ s with added feature of using a heuristic to guide traversal ) , and pagerank ( measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships ) . neo4j neo4j is a nosql database that supports both transactional and analytical processing of graph-based data . it is relatively new , schema-optional , acid-compliant , and supports various types of indexing and distributed computing . similar databases include microsoft cosmosdb and amazon neptune . neo4j uses cypher , a graph query language created in 2011 , and supports plugins like apoc for additional functions and the graph data science plugin for running graph algorithms . docker compose is a tool for managing multi-container applications using a yaml configuration file . it allows starting , stopping , and scaling services with a single command , ensuring consistent environments . interaction is mostly through the command line , and .env files help manage environment variables across platforms . in neo4j , relationships are directed . docker compose : docker compose supports multi-container management with a declarative setup using a yaml file ( docker-compose.yaml ) to define services , volumes , and networks . a single command can start , stop , or scale multiple services at once , ensuring a consistent environment . interaction is mostly through the command line , and .env files store environment variables to keep settings separate across platforms ( .env.local , .env.dev , .env.prod ) . port numbers maximum port number is 65535. range of ports for system services ( need root access ) is 0-1023. http : 80 https : 443 ssh : 22 ftp : 21 retrieval augmented generation ( rag ) the outputs are contextual to the input , in other words the pdf of the class notes is contextual elements and augmenting what is already in the model with that context . ansley searching in database systems searching is a common operation in database systems . in sql , the select statement is one of the most versatile and complex operations . baseline efficiency : linear search linear search starts at the beginning of a list and proceeds element by element until : 1. the target is found . 2. the last element is reached without finding the target . key terms - record : a collection of attribute values for a single entity instance ( a row in a table ) . - collection : a set
element by element until : 1. the target is found . 2. the last element is reached without finding the target . key terms - record : a collection of attribute values for a single entity instance ( a row in a table ) . - collection : a set of records of the same entity type ( a table ) . - search key : a value from an attribute used to search . it can be a single or multiple attributes . lists of records - memory usage : if each record takes x bytes , for n records , the total memory required is n * x bytes . - contiguously allocated list : all n * x bytes are allocated as a single memory block . - linked list : each record takes x bytes , plus additional memory for one or two addresses to link records together . contiguous vs. linked lists contiguous allocated list ( array ) - memory is allocated as a single block . - faster for random access . - slower for insertions , except at the end . linked list - records are linked by memory addresses . - extra storage is needed for the address . - faster for insertions anywhere in the list . - slower for random access . insertion examples - array : inserting after the second record requires moving 5 records to make space . - linked list : inserting after the second record does not require moving other records . observations - arrays : fast for random access , but slow for insertions . - linked lists : slow for random access , but fast for insertions . binary search - input : a sorted array and a target value . - output : the index of the target , or an indication that it is not found . def binary_search ( arr , target ) : left , right = 0 , len ( arr ) - 1 while left < = right : mid = ( left + right ) // 2 if arr [ mid ] == target : return mid elif arr [ mid ] < target : left = mid + 1 else : right = mid - 1 return -1 - example : in a sorted array , the target `` a '' is located by adjusting left or right based on the midpoint comparison . time complexity - linear search : - best case : o ( 1 ) ( target found at the first element ) . - worst case : o ( n ) ( target is not found ) . - binary search : - best case : o ( 1 ) ( target found at midpoint ) . - worst case : o ( log2 n ) ( target not found ) . searching in databases - storing by id : fast for searching a specific id . - searching by special value
- best case : o ( 1 ) ( target found at midpoint ) . - worst case : o ( log2 n ) ( target not found ) . searching in databases - storing by id : fast for searching a specific id . - searching by special value : linear scan is required , which is inefficient . issue : storing data by both id and special value - it is not possible to store data sorted by both id and special value at the same time due to space inefficiency . solution : external data structures - option 1 : array of tuples ( specialval , rownumber ) sorted by specialval . binary search can quickly find a specialval , but inserting is slow because the array is sorted . - option 2 : linked list of tuples ( specialval , rownumber ) sorted by specialval . insertion is fast , but searching requires a linear scan . what about fast insert and search ? a binary search tree ( bst ) can provide both fast insertion and searching . in a bst : - each node in the left subtree is less than the parent node . - each node in the right subtree is greater than the parent node . relational databases & distributed systems benefits of the relational model ( mostly ) standard data model and query language acid compliance works well with highly structured data can handle large amounts of data well understood , extensive tooling and expertise relational database performance enhancements indexing direct storage control column-oriented vs row-oriented storage query optimization caching/prefetching materialized views precompiled stored procedures data replication and partitioning transaction processing a transaction is a sequence of one or more crud operations performed as a single logical unit of work : either the entire sequence succeeds ( commit ) or the entire sequence fails ( rollback or abort ) benefits : data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit either fully executed or no parts are executed consistency a transaction takes a database from one consistent state to another consistent state : all data meets integrity constraints isolation when two transactions t1 and t2 execute simultaneously , they should not affect each other : if both t1 and t2 are reading data : no problem if t1 reads data that t2 may be writing , potential issues include : dirty read : t1 reads a row modified by uncommitted t2 non-repeatable read : t1 executes same query twice but gets different values because t2 committed changes phantom reads : t1 is running while t2 adds/deletes rows from the set t1 is using durability once a transaction completes and commits successfully , changes are permanent changes are preserved even during system failure example transaction : money transfer sqlcopydelimiter // create procedure transfer ( in sender_id int , in receiver_id int , in amount decimal ( 10,2 )
from the set t1 is using durability once a transaction completes and commits successfully , changes are permanent changes are preserved even during system failure example transaction : money transfer sqlcopydelimiter // create procedure transfer ( in sender_id int , in receiver_id int , in amount decimal ( 10,2 ) ) begin declare rollback_message varchar ( 255 ) default 'transaction rolled back : insufficient funds ' ; declare commit_message varchar ( 255 ) default 'transaction committed successfully ' ; -- start the transaction start transaction ; -- attempt to debit money from sender update accounts set balance = balance - amount where account_id = sender_id ; -- attempt to credit money to receiver update accounts set balance = balance + amount where account_id = receiver_id ; -- check if there are sufficient funds in sender account if ( select balance from accounts where account_id = sender_id ) < 0 then -- roll back the transaction if insufficient funds rollback ; signal sqlstate '45000 ' -- 45000 is unhandled , user-defined error set message_text = rollback_message ; else -- log the transactions if sufficient funds insert into transactions ( account_id , amount , transaction_type ) values ( sender_id , -amount , 'withdrawal ' ) ; insert into transactions ( account_id , amount , transaction_type ) values ( receiver_id , amount , 'deposit ' ) ; -- commit the transaction commit ; select commit_message as 'result ' ; end if ; end // delimiter ; limitations of relational databases schemas evolve over time not all applications need full acid compliance join operations can be expensive semi-structured or unstructured data ( json , xml ) not handled optimally horizontal scaling challenges performance limitations for real-time , low-latency systems scalability : vertical vs. horizontal conventional wisdom : scale vertically ( up ) with bigger , more powerful systems only scale horizontally ( out ) when high-availability necessitates distributed computing considerations : vertical scaling is simpler ( no architecture changes ) but has practical and financial limits modern systems make horizontal scaling more manageable distributed systems a distributed system is `` a collection of independent computers that appear to its users as one computer . '' -andrew tanenbaum characteristics : computers operate concurrently computers fail independently no shared global clock distributed storage approaches distributed data stores data stored on multiple nodes , typically replicated each block of data is available on n nodes can be relational or non-relational : mysql and postgresql support replication and sharding cockroachdb is a newer distributed sql database many nosql systems support distribution models important consideration : network partitioning is inevitable network failures , system failures will occur overall system needs to be partition tolerant ( continue running despite network partitions ) the cap theorem the cap theorem states that a distributed data store can not simultaneously provide more than two of these three guarantees : consistency : every read receives most recent write or an error availability : every request receives a non-error response ( but not guaranteed to
partitions ) the cap theorem the cap theorem states that a distributed data store can not simultaneously provide more than two of these three guarantees : consistency : every read receives most recent write or an error availability : every request receives a non-error response ( but not guaranteed to contain most recent write ) partition tolerance : system operates despite arbitrary network issues database view of cap trade-offs consistency + availability system always responds with latest data every request gets a response may not handle network partitions well consistency + partition tolerance if system responds with data , it 's always the latest otherwise , data request is dropped sacrifices some availability availability + partition tolerance system always responds may not provide absolute latest data most common choice for distributed systems cap in reality what it really means : if you can not limit fault numbers , requests can go to any server , and you insist on serving every request , then consistency is impossible common interpretation : you must always sacrifice one : consistency , availability , or partition tolerance nosql & key-value databases distributed databases and concurrency models pessimistic concurrency ( acid ) focuses on `` data safety '' assumes transactions need protection from other transactions conflicts prevented by locking resources until transaction completion uses both read and write locks analogy : borrowing a library book - if you have it , no one else can use it more details : how databases guarantee isolation optimistic concurrency transactions do not obtain locks on data for reading/writing assumes conflicts are unlikely to occur implementation : add timestamp and version columns to tables read these when changing data check at transaction end if another transaction modified them works well for : low-conflict systems ( backups , analytical dbs ) read-heavy systems systems that can tolerate rollbacks and retries less efficient for high-conflict systems where locking may be preferable nosql overview term `` nosql '' first used in 1998 by carlo strozzi for a relational database without sql modern meaning : `` not only sql '' ( sometimes interpreted as non-relational dbs ) developed partly as a response to processing unstructured web-based data brief history of non-relational databases cap theorem you can have 2 , but not all 3 , of the following : consistency : every user has an identical view of data at any given instant availability : database remains operational during failures partition tolerance : database maintains operations despite network failures between system segments note : consistency in cap differs from consistency in acid cap trade-offs consistency + availability : system always responds with latest data but may not handle network partitions consistency + partition tolerance : system responds with latest data or drops the request availability + partition tolerance : system always responds but may not provide the absolute latest data base model ( acid alternative for distributed systems ) basically available : data is available but might be inconsistent or changing soft state :
: system responds with latest data or drops the request availability + partition tolerance : system always responds but may not provide the absolute latest data base model ( acid alternative for distributed systems ) basically available : data is available but might be inconsistent or changing soft state : system state may change without input due to eventual consistency data stores do n't require write-consistency replicas do n't require mutual consistency eventual consistency : system will eventually become consistent when writes stop key-value databases core design principles simplicity extremely simple data model ( key = value ) supports basic crud operations and api creation much simpler than relational tables speed typically deployed as in-memory databases o ( 1 ) retrieval operations using hash tables or similar structures no complex queries or joins to slow things down scalability easy horizontal scaling by adding nodes uses eventual consistency in distributed environments use cases data science applications eda/experimentation results store intermediate results from data preprocessing a/b testing results without production db impact feature store low-latency retrieval for model training and prediction model monitoring store real-time performance metrics software engineering applications session information storage fast single-operation retrieval and storage user profiles & preferences language , timezone , ui preferences shopping cart data cross-browser/device availability caching layer front-end for disk-based databases redis ( remote directory server ) overview open source , in-memory database sometimes called a `` data structure store '' primarily a kv store , but supports other models : graph , spatial , full text search , vector , time series top-ranked key-value store according to db-engines.com key features in-memory but supports data durability through : disk snapshots at intervals append-only file journal for roll-forward recovery originally developed in 2009 in c++ performance : > 100,000 set operations/second rich command set limitations : no complex data , no secondary indexes , lookup by key only data types keys : usually strings ( can be any binary sequence ) values : strings lists ( linked lists ) sets ( unique unsorted string elements ) sorted sets hashes ( string → string ) geospatial data json redis databases 16 databases by default ( numbered 0-15 ) no other naming conventions interaction through commands or language libraries redis data types and commands strings simplest data type : maps string to string can store text , serialized objects , binary arrays use cases : caching html/css/js fragments configuration/user settings token management page view counting rate limiting string commands copyset /path/to/resource 0 set user:1 `` john doe '' get /path/to/resource exists user:1 del user:1 keys user * select 5 # select database 5 incr somevalue # increment by 1 incrby somevalue 10 # increment by 10 decr somevalue # decrement by 1 decrby somevalue 5 # decrement by 5 setnx key value # set only if key does n't exist hashes value is a collection of field-value pairs use cases : represent basic objects/structures session information management user/event tracking active session tracking hash commands copyhset bike:1 model
somevalue # decrement by 1 decrby somevalue 5 # decrement by 5 setnx key value # set only if key does n't exist hashes value is a collection of field-value pairs use cases : represent basic objects/structures session information management user/event tracking active session tracking hash commands copyhset bike:1 model demios brand ergonom price 1971 hget bike:1 model hget bike:1 price hgetall bike:1 hmget bike:1 model price weight hincrby bike:1 price 100 lists value is a linked list of string values use cases : stacks and queues implementation message passing queues logging systems social media streams/feeds chat application message history batch processing task queues list commands copy # queue operations lpush bikes : repairs bike:1 lpush bikes : repairs bike:2 rpop bikes : repairs # stack operations lpush bikes : repairs bike:1 lpush bikes : repairs bike:2 lpop bikes : repairs # other operations llen mylist lrange mylist 0 3 # elements from index 0 to 3 lrange mylist 0 0 # first element lrange mylist -2 -1 # last two elements sets unordered collection of unique strings use cases : track unique items ( ip addresses ) primitive relations access control lists social network friends/group membership supports set operations set commands copysadd ds4300 `` mark '' sadd ds4300 `` sam '' sadd cs3200 `` nick '' sadd cs3200 `` sam '' sismember ds4300 `` mark '' # check membership scard ds4300 # count elements sinter ds4300 cs3200 # intersection sdiff ds4300 cs3200 # difference srem ds4300 `` mark '' # remove element srandmember ds4300 # random member json type full support of json standard uses jsonpath syntax for parsing/navigating stored internally in binary tree structure for fast sub-element access redis setup guide pre-requisites docker desktop installed jetbrains datagrip installed step 1 : find the redis image open docker desktop use the built-in search to find the redis image click `` run '' step 2 : configure & run the container give the new container a name enter 6379 in the host port field ( standard redis port ) click `` run '' allow docker time to download and start redis step 3 : set up data source in datagrip start datagrip create a new redis data source by either : clicking the `` + '' icon in the database explorer selecting `` new '' from the file menu step 4 : configure the data source give the data source a name install drivers if needed ( a message will appear above `` test connection '' if required ) test the connection to redis click `` ok '' if connection test was successful notes redis can store various data types : strings , numbers , json objects , binary objects , etc . if drivers are n't already installed , datagrip will show a message above the `` test connection '' button redis-py guide overview redis-py is the official python client for redis , maintained by redis , inc. github : redis/redis-py installation : pip install redis storage capability :
, etc . if drivers are n't already installed , datagrip will show a message above the `` test connection '' button redis-py guide overview redis-py is the official python client for redis , maintained by redis , inc. github : redis/redis-py installation : pip install redis storage capability : handles strings , numbers , json objects , and binary data connection pythoncopyimport redis # connect to redis server redis_client = redis.redis ( host='localhost ' , # for docker : localhost or 127.0.0.1 port=6379 , # default redis port db=2 , # database number ( 0-15 ) decode_responses=true # converts byte responses to strings ) key commands by data type string operations pythoncopy # basic operations redis_client.set ( 'clickcount : /abc ' , 0 ) value = redis_client.get ( 'clickcount : /abc ' ) redis_client.incr ( 'clickcount : /abc ' ) # multiple operations redis_client.mset ( { 'key1 ' : 'val1 ' , 'key2 ' : 'val2 ' , 'key3 ' : 'val3 ' } ) values = redis_client.mget ( 'key1 ' , 'key2 ' , 'key3 ' ) # returns [ 'val1 ' , 'val2 ' , 'val3 ' ] available string commands : write : set ( ) , mset ( ) , setex ( ) , msetnx ( ) , setnx ( ) read : get ( ) , mget ( ) , getex ( ) , getdel ( ) numeric : incr ( ) , decr ( ) , incrby ( ) , decrby ( ) text : strlen ( ) , append ( ) list operations pythoncopy # create and populate list redis_client.rpush ( 'names ' , 'mark ' , 'sam ' , 'nick ' ) all_names = redis_client.lrange ( 'names ' , 0 , -1 ) # returns [ 'mark ' , 'sam ' , 'nick ' ] available list commands : left operations : lpush ( ) , lpop ( ) right operations : rpush ( ) , rpop ( ) management : lset ( ) , lrem ( ) , lrange ( ) , llen ( ) , lpos ( ) hash operations pythoncopy # create and populate hash redis_client.hset ( 'user-session:123 ' , mapping= { 'first ' : 'sam ' , 'last ' : 'uelle ' , 'company ' : 'redis ' , 'age ' : 30 } ) # get all hash fields user_data = redis_client.hgetall ( 'user-session:123 ' ) available hash commands : basic : hset ( ) , hget ( ) , hgetall ( ) management : hkeys ( ) , hdel ( ) , hexists ( ) , hlen ( ) , hstrlen ( ) pipelines reduce network overhead by batching multiple commands : pythoncopy # create pipeline pipe = redis_client.pipeline ( ) # set multiple values in one execution for i in range ( 5 ) : pipe.set ( f '' seat : { i } '' , f '' # { i } '' ) results = pipe.execute ( ) # [ true , true
pipeline pipe = redis_client.pipeline ( ) # set multiple values in one execution for i in range ( 5 ) : pipe.set ( f '' seat : { i } '' , f '' # { i } '' ) results = pipe.execute ( ) # [ true , true , true , true , true ] # chain commands results = redis_client.pipeline ( ) .get ( `` seat:0 '' ) .get ( `` seat:3 '' ) .get ( `` seat:4 '' ) .execute ( ) # results : [ ' # 0 ' , ' # 3 ' , ' # 4 ' ] redis in ml/data science redis serves as an essential component in ml architectures : feature stores vector databases model serving caching layers real-time processing resources full redis command list redis-py documentation document database a document database is a non-relational database that stores data as structured documents , usually in json . they are designed to be simple , flexible , and scalable . what is json ? json ( javascript object notation ) is a lightweight data-interchange format that is easy for humans to read and write , and easy for machines to parse and generate . json is built on two structures : 1. a collection of name/value pairs ( e.g. , object , record , dictionary , hash table , associative array ) . 2. an ordered list of values ( e.g. , array , vector , list , sequence ) . these two structures are supported by almost all modern programming languages , making json an ideal data interchange format . json syntax binary json ? bson bson ( binary json ) is a binary-encoded serialization of a json-like document structure . - supports extended types like date and binarydata . - lightweight to minimize space overhead . - designed to be easily traversed , which is important for document databases . - efficient encoding and decoding . - supported by many programming languages . xml ( extensible markup language ) xml was the precursor to json as a data exchange format . - xml + css is used to create web pages that separate content and formatting . - xml is structurally similar to html , but the tag set is extensible . xml-related tools/technologies - xpath : a syntax for retrieving specific elements from an xml document . - xquery : a query language for xml documents ( similar to sql ) . - dtd : a language to define the structure of an xml document . - xslt : a tool to transform xml into other formats , including html . why document databases ? document databases solve the impedance mismatch problem between object-oriented programming ( oop ) and how relational databases structure data . in oop , inheritance and composition of types are used . however , saving complex objects in a relational database requires deconstructing them . document databases allow the structure of a document to be
impedance mismatch problem between object-oriented programming ( oop ) and how relational databases structure data . in oop , inheritance and composition of types are used . however , saving complex objects in a relational database requires deconstructing them . document databases allow the structure of a document to be self-describing , making them a natural fit for applications that use json/xml for data transport . mongodb mongodb started in 2007 after doubleclick was acquired by google , and three of its veterans realized the limitations of relational databases for serving > 400,000 ads per second . mongodb was short for `` humongous database . '' mongodb atlas was released in 2016 , offering document databases as a service . mongodb structure - database - collection a - collection b - collection c - document 1 - document 2 - document 3 mongodb documents - no predefined schema for documents . - every document in a collection can have a different schema . relational vs mongo/document db rdbms vs mongodb : - database : database - table/view : collection - row : document - column : field - index : index - join : embedded document - foreign key : reference mongodb features : - rich query support : full support for crud operations . - indexing : supports primary and secondary indexes on document fields . - replication : supports replica sets with automatic failover . - load balancing : built-in load balancing . mongodb versions : - mongodb atlas : fully managed mongodb service ( dbaas ) . - mongodb enterprise : subscription-based , self-managed version . - mongodb community : source-available , free-to-use , self-managed version . interacting with mongodb : - mongosh : mongodb shell ( cli tool ) . - mongodb compass : free , open-source gui for working with mongodb . - datagrip and other 3rd-party tools : libraries to interface with mongodb for various languages ( e.g. , pymongo for python , mongoose for javascript ) . mongodb community edition in docker : - create a container and map the host : container port 27017 . - provide an initial username and password for the superuser . mongodb compass : gui tool for interacting with mongodb . download and install from the official website . load mflix sample data set : 1. create a new database called mflix . 2. download and unzip the mflix sample dataset . 3. import json files for users , theaters , movies , and comments into new collections in the mflix database . creating a database and collection : - use mongosh to interact with mongodb . - find queries in mongodb are similar to sql select statements . example : to select all users : - ` db.users.find ( ) ` to filter users by name : - ` db.users.find ( { `` name '' : `` davos seaworth '' } ) ` to filter movies released in 2010 and having a specific rating or genre
statements . example : to select all users : - ` db.users.find ( ) ` to filter users by name : - ` db.users.find ( { `` name '' : `` davos seaworth '' } ) ` to filter movies released in 2010 and having a specific rating or genre : - ` db.movies.find ( { `` year '' : 2010 , $ or : [ { `` awards.wins '' : { $ gte : 5 } } , { `` genres '' : `` drama '' } ] } ) ` count documents in a collection : - to count documents , use ` countdocuments ( ) ` method . example : to count movies from 2010 with at least 5 awards or with a drama genre : - ` db.movies.countdocuments ( { `` year '' : 2010 , $ or : [ { `` awards.wins '' : { $ gte : 5 } } , { `` genres '' : `` drama '' } ] } ) ` pymongo pymongo is a python library for interfacing with mongodb instances . pymongo pymongo is a python library for interfacing with mongodb instances . getting a database and collection from pymongo import mongoclient client = mongoclient ( 'mongodb : //user_name : pw @ localhost:27017 ' ) db = client [ 'ds4300 ' ] # or client.ds4300 collection = db [ 'mycollection ' ] # or db.mycollection inserting a single document db = client [ 'ds4300 ' ] collection = db [ 'mycollection ' ] post = { `` author '' : `` mark '' , `` text '' : `` mongodb is cool ! `` , `` tags '' : [ `` mongodb '' , `` python '' ] } post_id = collection.insert_one ( post ) .inserted_id print ( post_id ) find all movies from 2000 from bson.json_util import dumps # find all movies released in 2000 movies_2000 = db.movies.find ( { `` year '' : 2000 } ) # print results print ( dumps ( movies_2000 , indent=2 ) ) jupyter time - activate your ds4300 conda or venv python environment . - install pymongo with ` pip install pymongo ` . - install jupyter lab in your python environment with ` pip install jupyterlab ` . - download and unzip this zip file — it contains 2 jupyter notebooks . - in terminal , navigate to the folder where you unzipped the files , and run ` jupyter lab ` . graph databases and graph theory what is a graph database ? data model based on the graph data structure composed of nodes ( vertices ) and edges ( relationships ) edges connect nodes each element is uniquely identified each can contain properties ( e.g. , name , occupation ) supports graph-oriented operations : traversals shortest path algorithms many other specialized queries real-world graph applications social networks social media platforms ( instagram , facebook ) modeling social interactions in psychology and sociology the web a large graph of
identified each can contain properties ( e.g. , name , occupation ) supports graph-oriented operations : traversals shortest path algorithms many other specialized queries real-world graph applications social networks social media platforms ( instagram , facebook ) modeling social interactions in psychology and sociology the web a large graph of pages ( nodes ) connected by hyperlinks ( edges ) chemical and biological data systems biology , genetics chemical interaction relationships labeled property graph model composed of node ( vertex ) objects and relationship ( edge ) objects labels : used to mark nodes as part of a group properties : key-value pair attributes that can exist on both nodes and relationships structure rules : nodes with no relationships are permitted edges not connected to nodes are not allowed graph example components 2 labels : person car 4 relationship types : drives owns lives_with married_to properties : attributes attached to nodes and relationships paths in graphs a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated . example : valid path : 1 → 2 → 6 → 5 invalid path : 1 → 2 → 6 → 2 → 3 ( node 2 repeats ) graph types and properties connected vs. disconnected connected : a path exists between any two nodes in the graph disconnected : contains isolated components with no paths between them weighted vs. unweighted weighted : edges have a weight property ( important for certain algorithms ) unweighted : no weights assigned to edges directed vs. undirected directed : relationships ( edges ) define a specific start and end node undirected : relationships have no direction cyclic vs. acyclic cyclic : graph contains at least one cycle acyclic : graph contains no cycles sparse vs. dense sparse : relatively few edges compared to the maximum possible dense : has many edges , approaching the maximum possible number trees a tree is a special type of graph : connected acyclic undirected each node ( except the root ) has exactly one parent graph algorithms overview pathfinding algorithms definition : find the shortest path between nodes ( fewest edges or lowest weight ) use case : monitor network efficiency and resiliency using average shortest path variants : minimum spanning tree , cycle detection , max/min flow algorithms search approaches : bfs ( breadth-first search ) : explores all neighbors before moving deeper dfs ( depth-first search ) : explores as far as possible along branches before backtracking centrality & community detection centrality : identifies `` important '' nodes in a network ( e.g. , social network influencers ) community detection : evaluates clustering/partitioning of nodes and their cohesion famous graph algorithms dijkstra 's algorithm : single-source shortest path for positively weighted graphs a algorithm * : enhanced dijkstra 's that uses heuristics to guide traversal pagerank : measures node importance based on incoming relationships and their sources neo4j type : graph database system supporting transactional and analytical processing classification
graph algorithms dijkstra 's algorithm : single-source shortest path for positively weighted graphs a algorithm * : enhanced dijkstra 's that uses heuristics to guide traversal pagerank : measures node importance based on incoming relationships and their sources neo4j type : graph database system supporting transactional and analytical processing classification : nosql database with schema-optional design features : various indexing capabilities acid compliance distributed computing support similar systems : microsoft cosmos db , amazon neptune this optimized format eliminates the unnecessary numbering ( 15-22 ) , organizes content into logical sections , and uses consistent formatting that llms can easily process and understand . { `` cells '' : [ { `` cell_type '' : `` markdown '' , `` id '' : `` 02ead21f-dc3b-4536-aa57-f2ead5395348 '' , `` metadata '' : { } , `` source '' : [ `` # mongodb aggregation examples '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 66da7d8d-4c4f-4c60-8e22-d0c023c35a0f '' , `` metadata '' : { } , `` source '' : [ `` - ensure you have pymongo installed before running cells in this notebook '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 5e29baf9-89a0-4618-9f91-2c8087128b7d '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` import pymongo\n '' , `` from bson.json_util import dumps\n '' , `` import pprint\n '' , `` \n '' , `` # -- > update the uri with your username and password < -- \n '' , `` \n '' , `` uri = \ '' mongodb : //mark : abc123 @ localhost:27017/\ '' \n '' , `` client = pymongo.mongoclient ( uri ) \n '' , `` mflixdb = client.mflix\n '' , `` demodb = client.demodb '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 1565e788-2c72-4ab6-a5b3-36e05334233b '' , `` metadata '' : { } , `` source '' : [ `` # # about aggregates in pymongo\n '' , `` \n '' , `` - aggregation uses _pipelines_.\n '' , `` - a * * pipeline * * is a sequence of stages through which documents proceed.\n '' , `` - some of the different stages that can be used are : \n '' , `` - match\n '' , `` - project\n '' , `` - sort\n '' , `` - limit\n '' , `` - unwind\n '' , `` - group\n '' , `` - lookup '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 921429f5-3125-475e-b0af-a871b9a0799e '' , `` metadata '' : { } , `` source '' : [ `` # # # $ match '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 1a9f528f-5ffa-40fa-99b6-5a3bc36b3e33 ''
'' : `` 921429f5-3125-475e-b0af-a871b9a0799e '' , `` metadata '' : { } , `` source '' : [ `` # # # $ match '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 1a9f528f-5ffa-40fa-99b6-5a3bc36b3e33 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ match\ '' : { \ '' year\ '' : { \ '' $ lte\ '' : 1920 } } } , \n '' , `` ] ) \n '' , `` \n '' , `` print ( dumps ( c , indent=4 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` e180713d-92ec-4392-ba99-6c5a96775903 '' , `` metadata '' : { } , `` source '' : [ `` # # # match and project '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` cc479ecc-7241-49d7-9cc2-5a57cd40bb5c '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ match\ '' : { \ '' year\ '' : { \ '' $ lte\ '' : 1920 } } } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' :0 , \ '' title\ '' : 1 , \ '' cast\ '' : 1 } } , \n '' , `` ] ) \n '' , `` \n '' , `` print ( dumps ( c , indent=4 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 7f7d7755-e9fa-4061-ac98-5f31b75201f2 '' , `` metadata '' : { } , `` source '' : [ `` # # # match project limit and sort '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` d7e42c04-668e-4dd6-abf7-7a081c126c13 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ match\ '' : { \ '' year\ '' : { \ '' $ lte\ '' : 1920 } } } , \n '' , `` { \ '' $ sort\ '' : { \ '' title\ '' : 1 } } , \n '' , `` { \ '' $ limit\ '' : 5 } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' :0 , \ '' title\
'' $ sort\ '' : { \ '' title\ '' : 1 } } , \n '' , `` { \ '' $ limit\ '' : 5 } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' :0 , \ '' title\ '' : 1 , \ '' cast\ '' : 1 } } , \n '' , `` ] ) \n '' , `` \n '' , `` print ( dumps ( c , indent=4 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 6e58c4a2-ff42-48a1-8493-aba643b7592d '' , `` metadata '' : { } , `` source '' : [ `` # # # unwind '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 02ac7ecc-69ee-45cb-83ef-b271afaccbe1 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ match\ '' : { \ '' year\ '' : { \ '' $ lte\ '' : 1920 } } } , \n '' , `` { \ '' $ sort\ '' : { \ '' imdb.rating\ '' : -1 } } , \n '' , `` { \ '' $ limit\ '' : 5 } , \n '' , `` { \ '' $ unwind\ '' : \ '' $ cast\ '' } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' :0 , \ '' title\ '' : 1 , \ '' cast\ '' : 1 , \ '' rating\ '' : \ '' $ imdb.rating\ '' } } , \n '' , `` ] ) \n '' , `` \n '' , `` print ( dumps ( c , indent=4 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 4a2edddc-422e-4f67-8ddb-87343c5eb004 '' , `` metadata '' : { } , `` source '' : [ `` # # grouping '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 4e3b2ef1-681c-4cf4-baba-7d1399de1dd1 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # what is the average imdb rating of all movies by year ? sort the data by year.\n '' , `` \n '' , `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ group\ '' : { \ '' _id\ '' : { \ '' release year\ '' : \ '' $ year\ '' } , \ '' avg rating\ '' : { \ '' $ avg\ '' : \ '' $ imdb.rating\ '' }
\n '' , `` { \ '' $ group\ '' : { \ '' _id\ '' : { \ '' release year\ '' : \ '' $ year\ '' } , \ '' avg rating\ '' : { \ '' $ avg\ '' : \ '' $ imdb.rating\ '' } } } , \n '' , `` { \ '' $ sort\ '' : { \ '' _id\ '' : 1 } } \n '' , `` \n '' , `` ] ) \n '' , `` print ( dumps ( c , indent = 2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 665047a1-87cb-422f-91ae-645714020a32 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # what is the average imdb rating of all movies by year ? sort the data by avg rating in decreasing order.\n '' , `` \n '' , `` c = mflixdb.movies.aggregate ( [ \n '' , `` { \ '' $ group\ '' : { \ '' _id\ '' : { \ '' release year\ '' : \ '' $ year\ '' } , \ '' avg rating\ '' : { \ '' $ avg\ '' : \ '' $ imdb.rating\ '' } } } , \n '' , `` { \ '' $ sort\ '' : { \ '' avg rating\ '' : -1 , \ '' _id\ '' : 1 } } \n '' , `` \n '' , `` ] ) \n '' , `` print ( dumps ( c , indent = 2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` f3bb3303-7cce-44f2-a9bb-7503ee2741e1 '' , `` metadata '' : { } , `` source '' : [ `` # # lookup '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 6f869bbe-701b-4fb3-980f-be9f1879b1ed '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` data = demodb.customers.aggregate ( [ \n '' , `` { \n '' , `` \ '' $ lookup\ '' : { \n '' , `` \ '' from\ '' : \ '' orders\ '' , \n '' , `` \ '' localfield\ '' : \ '' custid\ '' , \n '' , `` \ '' foreignfield\ '' : \ '' custid\ '' , \n '' , `` \ '' as\ '' : \ '' orders\ '' \n '' , `` } \n '' , `` } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' : 0 , \ '' address\ '' : 0 } } \n '' , `` ] ) \n
: \ '' orders\ '' \n '' , `` } \n '' , `` } , \n '' , `` { \ '' $ project\ '' : { \ '' _id\ '' : 0 , \ '' address\ '' : 0 } } \n '' , `` ] ) \n '' , `` print ( dumps ( data , indent = 2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 7e24db4a-38f0-4464-ab8b-19d675af29db '' , `` metadata '' : { } , `` source '' : [ `` # # reformatting queries '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 1d7f801c-05e1-4c05-baad-673c36302d20 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` match = { \ '' $ match\ '' : { \ '' year\ '' : { \ '' $ lte\ '' : 1920 } } } \n '' , `` limit = { \ '' $ limit\ '' : 5 } \n '' , `` project = { \ '' $ project\ '' : { \ '' _id\ '' :0 , \ '' title\ '' : 1 , \ '' cast\ '' : 1 , \ '' rating\ '' : \ '' $ imdb.rating\ '' } } \n '' , `` \n '' , `` agg = mflixdb.movies.aggregate ( [ match , limit , project ] ) \n '' , `` print ( dumps ( agg , indent=2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 76e9276d-938d-45e3-92f4-f2daa42ce704 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ ] } ] , `` metadata '' : { `` kernelspec '' : { `` display_name '' : `` python 3 ( ipykernel ) '' , `` language '' : `` python '' , `` name '' : `` python3 '' } , `` language_info '' : { `` codemirror_mode '' : { `` name '' : `` ipython '' , `` version '' : 3 } , `` file_extension '' : `` .py '' , `` mimetype '' : `` text/x-python '' , `` name '' : `` python '' , `` nbconvert_exporter '' : `` python '' , `` pygments_lexer '' : `` ipython3 '' , `` version '' : `` 3.11.9 '' } } , `` nbformat '' : 4 , `` nbformat_minor '' : 5 } { `` cells '' : [ { `` cell_type '' : `` markdown '' , `` id '' : `` 02ead21f-dc3b-4536-aa57-f2ead5395348 '' , `` metadata '' : { } , `` source '' : [ `` # mongodb + pymongo example queries '' ] } , { `` cell_type '' : `` markdown '' ,
[ { `` cell_type '' : `` markdown '' , `` id '' : `` 02ead21f-dc3b-4536-aa57-f2ead5395348 '' , `` metadata '' : { } , `` source '' : [ `` # mongodb + pymongo example queries '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 66da7d8d-4c4f-4c60-8e22-d0c023c35a0f '' , `` metadata '' : { } , `` source '' : [ `` - make sure your mongodb container is running\n '' , `` - make sure you have pymongo installed before running cells in this notebook . if not , use ` pip install pymongo ` . `` ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 5e29baf9-89a0-4618-9f91-2c8087128b7d '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` import pymongo\n '' , `` from bson.json_util import dumps\n '' , `` \n '' , `` # -- > update the uri with your username and password < -- \n '' , `` \n '' , `` uri = \ '' mongodb : //username : password @ localhost:27017\ '' \n '' , `` client = pymongo.mongoclient ( uri ) \n '' , `` mflixdb = client.mflix '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` e4ec02e0-7f92-40dc-a6db-c0bd0e2d5b32 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # setup demodb with 2 collections\n '' , `` demodb.customers.drop ( ) \n '' , `` demodb.orders.drop ( ) \n '' , `` \n '' , `` customers = [ \n '' , `` { \ '' custid\ '' : \ '' c13\ '' , \ '' name\ '' : \ '' t. cruise\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 201 main st.\ '' , \ '' city\ '' : \ '' st. louis , mo\ '' , \ '' zipcode\ '' : \ '' 63101\ '' } , \ '' rating\ '' : 750 } , \n '' , `` { \ '' custid\ '' : \ '' c25\ '' , \ '' name\ '' : \ '' m. streep\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 690 river st.\ '' , \ '' city\ '' : \ '' hanover , ma\ '' , \ '' zipcode\ '' : \ '' 02340\ '' } , \ '' rating\ '' : 690 } , \n '' , `` { \ '' custid\ '' : \ '' c31\ '' , \ '' name\ '' : \ '' b. pitt\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 360 mountain ave.\ '' , \ '' city\ '' : \ '' st. louis
`` { \ '' custid\ '' : \ '' c31\ '' , \ '' name\ '' : \ '' b. pitt\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 360 mountain ave.\ '' , \ '' city\ '' : \ '' st. louis , mo\ '' , \ '' zipcode\ '' : \ '' 63101\ '' } } , \n '' , `` { \ '' custid\ '' : \ '' c35\ '' , \ '' name\ '' : \ '' j. roberts\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 420 green st.\ '' , \ '' city\ '' : \ '' boston , ma\ '' , \ '' zipcode\ '' : \ '' 02115\ '' } , \ '' rating\ '' : 565 } , \n '' , `` { \ '' custid\ '' : \ '' c37\ '' , \ '' name\ '' : \ '' t. hanks\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 120 harbor blvd.\ '' , \ '' city\ '' : \ '' boston , ma\ '' , \ '' zipcode\ '' : \ '' 02115\ '' } , \ '' rating\ '' : 750 } , \n '' , `` { \ '' custid\ '' : \ '' c41\ '' , \ '' name\ '' : \ '' r. duvall\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' 150 market st.\ '' , \ '' city\ '' : \ '' st. louis , mo\ '' , \ '' zipcode\ '' : \ '' 63101\ '' } , \ '' rating\ '' : 640 } , \n '' , `` { \ '' custid\ '' : \ '' c47\ '' , \ '' name\ '' : \ '' s. loren\ '' , \ '' address\ '' : { \ '' street\ '' : \ '' via del corso\ '' , \ '' city\ '' : \ '' rome , italy\ '' } , \ '' rating\ '' : 625 } \n '' , `` ] \n '' , `` \n '' , `` orders = [ \n '' , `` { \ '' orderno\ '' : 1001 , \ '' custid\ '' : \ '' c41\ '' , \ '' order_date\ '' : \ '' 2017-04-29\ '' , \ '' ship_date\ '' : \ '' 2017-05-03\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 347 , \ '' qty\ '' : 5 , \ '' price\ '' : 19.99 } , { \ '' itemno\ '' : 193 , \ '' qty\ '' : 2 , \ '' price\ '' : 28.89 } ] } , \n '' , `` { \ '' orderno\ '' : 1002 , \ '' custid\ '' : \ '' c13\ '' , \ '' order_date\ '' : \ '' 2017-05-01\ '' ,
, \ '' qty\ '' : 2 , \ '' price\ '' : 28.89 } ] } , \n '' , `` { \ '' orderno\ '' : 1002 , \ '' custid\ '' : \ '' c13\ '' , \ '' order_date\ '' : \ '' 2017-05-01\ '' , \ '' ship_date\ '' : \ '' 2017-05-03\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 460 , \ '' qty\ '' : 95 , \ '' price\ '' : 100.99 } , { \ '' itemno\ '' : 680 , \ '' qty\ '' : 150 , \ '' price\ '' : 8.75 } ] } , \n '' , `` { \ '' orderno\ '' : 1003 , \ '' custid\ '' : \ '' c31\ '' , \ '' order_date\ '' : \ '' 2017-06-15\ '' , \ '' ship_date\ '' : \ '' 2017-06-16\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 120 , \ '' qty\ '' : 2 , \ '' price\ '' : 88.99 } , { \ '' itemno\ '' : 460 , \ '' qty\ '' : 3 , \ '' price\ '' : 99.99 } ] } , \n '' , `` { \ '' orderno\ '' : 1004 , \ '' custid\ '' : \ '' c35\ '' , \ '' order_date\ '' : \ '' 2017-07-10\ '' , \ '' ship_date\ '' : \ '' 2017-07-15\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 680 , \ '' qty\ '' : 6 , \ '' price\ '' : 9.99 } , { \ '' itemno\ '' : 195 , \ '' qty\ '' : 4 , \ '' price\ '' : 35.00 } ] } , \n '' , `` { \ '' orderno\ '' : 1005 , \ '' custid\ '' : \ '' c37\ '' , \ '' order_date\ '' : \ '' 2017-08-30\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 460 , \ '' qty\ '' : 2 , \ '' price\ '' : 99.98 } , { \ '' itemno\ '' : 347 , \ '' qty\ '' : 120 , \ '' price\ '' : 22.00 } , { \ '' itemno\ '' : 780 , \ '' qty\ '' : 1 , \ '' price\ '' : 1500.00 } , { \ '' itemno\ '' : 375 , \ '' qty\ '' : 2 , \ '' price\ '' : 149.98 } ] } , \n '' , `` { \ '' orderno\ '' : 1006 , \ '' custid\ '' : \ '' c41\ '' , \ '' order_date\ '' : \ '' 2017-09-02\ '' , \ '' ship_date\ '' : \ '' 2017-09-04\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 680 , \ '' qty\ '' : 51
\ '' custid\ '' : \ '' c41\ '' , \ '' order_date\ '' : \ '' 2017-09-02\ '' , \ '' ship_date\ '' : \ '' 2017-09-04\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 680 , \ '' qty\ '' : 51 , \ '' price\ '' : 25.98 } , { \ '' itemno\ '' : 120 , \ '' qty\ '' : 65 , \ '' price\ '' : 85.00 } , { \ '' itemno\ '' : 460 , \ '' qty\ '' : 120 , \ '' price\ '' : 99.98 } ] } , \n '' , `` { \ '' orderno\ '' : 1007 , \ '' custid\ '' : \ '' c13\ '' , \ '' order_date\ '' : \ '' 2017-09-13\ '' , \ '' ship_date\ '' : \ '' 2017-09-20\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 185 , \ '' qty\ '' : 5 , \ '' price\ '' : 21.99 } , { \ '' itemno\ '' : 680 , \ '' qty\ '' : 1 , \ '' price\ '' : 20.50 } ] } , \n '' , `` { \ '' orderno\ '' : 1008 , \ '' custid\ '' : \ '' c13\ '' , \ '' order_date\ '' : \ '' 2017-10-13\ '' , \ '' items\ '' : [ { \ '' itemno\ '' : 460 , \ '' qty\ '' : 20 , \ '' price\ '' : 99.99 } ] } \n '' , `` ] \n '' , `` \n '' , `` demodb.customers.insert_many ( customers ) \n '' , `` demodb.orders.insert_many ( orders ) \n '' , `` \n '' , `` numcustomers = demodb.customers.count_documents ( { } ) \n '' , `` numorders = demodb.orders.count_documents ( { } ) \n '' , `` \n '' , `` print ( f'there are { numcustomers } customers and { numorders } orders ' ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 4b977e9f-6cc5-4afb-91ed-fc7eec781c4e '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # the key ( _id ) attribute is automatically returned unless you explicitly say to remove it . \n '' , `` \n '' , `` # select name , rating from customers\n '' , `` data = demodb.customers.find ( { } , { \ '' name\ '' :1 , \ '' rating\ '' :1 } ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 38d4b619-5989-4962-b3e6-ec1893095d72 '' , `` metadata '' : { `` scrolled '' : true } ,
, `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 38d4b619-5989-4962-b3e6-ec1893095d72 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # now without the _id field . \n '' , `` \n '' , `` # select name , rating from customers\n '' , `` data = demodb.customers.find ( { } , { \ '' name\ '' :1 , \ '' rating\ '' :1 , \ '' _id\ '' :0 } ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 1be79501-7f8b-43af-a955-9e9f95f842f2 '' , `` metadata '' : { } , `` source '' : [ `` # # # all fields except specific ones returned '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 48385e0f-fe32-4abd-91a0-ef8e732046c3 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # for every customer , return all fields except _id and address.\n '' , `` \n '' , `` data = demodb.customers.find ( { } , { \ '' _id\ '' : 0 , \ '' address\ '' : 0 } ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` d929a632-57de-4de5-8049-6dada9027e6b '' , `` metadata '' : { } , `` source '' : [ `` # # equivalent to sql like operator '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 84e41240-5186-41f1-ba94-3935e333e255 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # select name , rating from customers where name like 't % '\n '' , `` \n '' , `` # regular expression explanation : \n '' , `` # ^ - match beginning of line\n '' , `` # t - match literal character t ( at the beginning of the line in this case ) \n '' , `` # . - match any single character except newline\n '' , `` # * - match zero or more occurrences of the previous character ( the . in this case ) \n '' , `` \n '' , `` data = demodb.customers.find ( { \ '' name\ '' : { \ '' $ regex\ '' : \ '' ^t . * \ '' } } , { \ '' _id\ '' : 0
character ( the . in this case ) \n '' , `` \n '' , `` data = demodb.customers.find ( { \ '' name\ '' : { \ '' $ regex\ '' : \ '' ^t . * \ '' } } , { \ '' _id\ '' : 0 , \ '' name\ '' : 1 , \ '' rating\ '' :1 } ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 95c0ac04-ab9d-4a34-850a-16137cd1fc1d '' , `` metadata '' : { } , `` source '' : [ `` # # sorting and limiting `` ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` e07bf0b9-10f1-4fae-97f0-19ff2afda9bf '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # select name , rating from customers order by rating limit 2\n '' , `` \n '' , `` data = demodb.customers.find ( { } , { \ '' _id\ '' : 0 , \ '' name\ '' : 1 , \ '' rating\ '' :1 } ) .sort ( \ '' rating\ '' ) .limit ( 2 ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 08236f13-3f94-47da-bd19-f931c59039e4 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # same as above , but sorting in desc order\n '' , `` \n '' , `` # select name , rating from customers order by rating desc limit 2\n '' , `` \n '' , `` data = demodb.customers.find ( { } , { \ '' _id\ '' : 0 , \ '' name\ '' : 1 , \ '' rating\ '' :1 } ) .sort ( \ '' rating\ '' , -1 ) .limit ( 2 ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 7ae8e685-34d5-49b3-af1e-5a0f5ed6186c '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # providing 2 sort keys ... \n '' , `` \n '' , `` data = demodb.customers.find ( { } , { \ '' _id\ '' : 0 , \ '' name\ '' : 1 , \ '' rating\ '' :1 } ) .sort ( { \ '' rating\ '' : -1 , \ '' name\ '' : 1 } ) .limit ( 2 ) \n '' , `` print ( dumps ( data , indent=2 ) )
, \ '' name\ '' : 1 , \ '' rating\ '' :1 } ) .sort ( { \ '' rating\ '' : -1 , \ '' name\ '' : 1 } ) .limit ( 2 ) \n '' , `` print ( dumps ( data , indent=2 ) ) '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 143f4349-d0cf-4812-a27c-f627f709d8ae '' , `` metadata '' : { } , `` source '' : [ `` # your turn with mflix db '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 3e6cbf79-7813-4df0-8386-00b4c133ba0a '' , `` metadata '' : { } , `` source '' : [ `` # # question 1 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : 16 , `` id '' : `` ca28ccf2-bff1-43be-a1b6-2c7f735e4c3b '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # how many users are there in the mflix database ? how many movies ? \n '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` e520f1c3-f9bf-4668-8d84-7ef605c5bd90 '' , `` metadata '' : { } , `` source '' : [ `` # # question 2 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 224682d3-5448-4fcd-83b0-b4938b083b24 '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # which movies have a rating of “ tv-g ” ? only return the title and year.\n '' , `` \n '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 2a86daef-c330-493d-92d5-38517d7c4381 '' , `` metadata '' : { } , `` source '' : [ `` # # question 3 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 566f4500-2eb5-4190-abdf-bfb9228d5c3f '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # which movies have a runtime of less than 20 minutes ? only return the title and runtime of each movie . \n '' , `` \n '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` 05d0bfeb-eb46-4fe0-a257-54b632f1c34f '' , `` metadata '' : { } , `` source '' : [ `` # # question 4 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 86821f3e-a95e-40fd-a8bb-3afb7078cc6b '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` #
question 4 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 86821f3e-a95e-40fd-a8bb-3afb7078cc6b '' , `` metadata '' : { } , `` outputs '' : [ ] , `` source '' : [ `` # how many theaters are in mn or ma ? \n '' , `` \n '' , `` `` ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` d3c90cac-74bc-4b69-9a08-b8118bf97bfd '' , `` metadata '' : { } , `` source '' : [ `` # # question 5 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 03c20e7f-2d72-47c5-b104-083cca731cb5 '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # give the names of all movies that have no comments yet . make sure the names are in alphabetical order . \n '' , `` \n '' ] } , { `` cell_type '' : `` markdown '' , `` id '' : `` dd98ae0a-0d11-4d33-a5a8-05f6a7da052e '' , `` metadata '' : { } , `` source '' : [ `` # # question 6 '' ] } , { `` cell_type '' : `` code '' , `` execution_count '' : null , `` id '' : `` 2c5e0923-d2ec-4e86-a85f-57c4ef523b6f '' , `` metadata '' : { `` scrolled '' : true } , `` outputs '' : [ ] , `` source '' : [ `` # return a list of movie titles and all actors from any movie with a title that contains the word 'four ' . \n '' , `` # sort the list by title . \n '' ] } ] , `` metadata '' : { `` kernelspec '' : { `` display_name '' : `` python 3 ( ipykernel ) '' , `` language '' : `` python '' , `` name '' : `` python3 '' } , `` language_info '' : { `` codemirror_mode '' : { `` name '' : `` ipython '' , `` version '' : 3 } , `` file_extension '' : `` .py '' , `` mimetype '' : `` text/x-python '' , `` name '' : `` python '' , `` nbconvert_exporter '' : `` python '' , `` pygments_lexer '' : `` ipython3 '' , `` version '' : `` 3.11.9 '' } } , `` nbformat '' : 4 , `` nbformat_minor '' : 5 }
