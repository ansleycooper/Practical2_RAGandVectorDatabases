tim for a binary search tree the order of inserting values affects the shape of the tree our goal with binary search trees is to minimize their height the ideal binary search tree is complete where each node above the last level has 2 subnodes but that is hard to maintain an avl tree is an approximately balanced binary search tree that maintains a balance factor value in each node to ensure balance a node is considered imbalanced if the absolute value of the difference in the height of its left subtree and the height of its right subtree is greater than 1 a node that is imbalanced is considered the node of imbalance and can be represented by alpha there are four potential cases where an avl tree can become unbalanced for leftleft imbalance case 1 the height of alphas left subtree is larger than the height of its right subtree and the most recently added node was to the left of its parent for leftright imbalance case 2 the height of alphas left subtree is larger than the height of its right subtree and the most recently added node was to the right of its parent for rightleft
right subtree and the most recently added node was to the left of its parent for leftright imbalance case 2 the height of alphas left subtree is larger than the height of its right subtree and the most recently added node was to the right of its parent for rightleft imbalance case 3 the height of alphas right subtree is larger than the height of its left subtree and the most recently added node was to the left of its parent for rightright imbalance case 4 the height of alphas right subtree is larger than the height of its left subtree and the most recently added node was to the right of its parent cases 1 and 4 and cases 2 and 3 are mirrors of each other for case 1 and case 2 we call alphas left child node a while for case 3 and 4 we call alphas right child node a for case 2 we call as right child node b and for case 3 we call as left child node b to rebalance case 1 we perform a single rotation by setting as right child to be alpha and moving as old right subtree to be
right child node a for case 2 we call as right child node b and for case 3 we call as left child node b to rebalance case 1 we perform a single rotation by setting as right child to be alpha and moving as old right subtree to be the left subtree of alpha to rebalance case 4 we perform a single rotation by setting as left child to be alpha and moving as old left subtree to be the right subtree of alpha to rebalance case 2 we first perform a single rotation with nodes a and b by setting alphas new left child to be node b setting node bs new left child to be node a and by setting as new right child to be node bs old left subtree once we perform the first rotation for case 2 the tree becomes case 1 where the old node b can be considered node a while alpha remains alpha the tree can be balanced using the same rotation needed to case 1 to rebalance case 3 we first perform a single rotation with nodes a and b by setting alphas new right child to be node b
old node b can be considered node a while alpha remains alpha the tree can be balanced using the same rotation needed to case 1 to rebalance case 3 we first perform a single rotation with nodes a and b by setting alphas new right child to be node b setting node bs new right child to be node a and by setting as new left child to be node bs old right subtree once we perform the first rotation for case 3 the tree becomes case 4 where the old node b can be considered node a while alpha remains alpha the tree can be balanced using the same rotation needed to case 4 when you insert a node into an avl tree you should update the heights on the path from the newly inserted node to the root node of the tree checking for imbalance as you go computer memory can be stored in a number of different locations in descending order of speed they are the cpu registers the l1 cache the l2 cache ram and sddhdd sddhdd is multiple orders of magnitude slower than ram but have lots of storage and are persistent the goal of
computer memory can be stored in a number of different locations in descending order of speed they are the cpu registers the l1 cache the l2 cache ram and sddhdd sddhdd is multiple orders of magnitude slower than ram but have lots of storage and are persistent the goal of database systems is to minimize the number of sddhdd accesses since a worst case binary search search is significantly faster than a single additional access of sddhdd data raw binary search trees are not good for this because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may
are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored btrees may also be useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several requirements first every path from the root to a leaf has the same length second if a node has n children it contains n1 keys third every node except the root is at least half full fourth the elements stored in a given subtree all have keys that are between the keys in the parent
the root to a leaf has the same length second if a node has n children it contains n1 keys third every node except the root is at least half full fourth the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant fifth the root has at least two children if it is not a leaf because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olog n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that are read from disk lookup in a btree is straightforward given a node to start from we use a
memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that are read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer to follow from the current node insertion and deletion from a btree are more complicated for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one deletion works
to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that node is no longer at least half full the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possibly causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one hash tables effectively function like a python dictionary the hash table itself is a 1dimensional array each index stores a list which contains the
are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one hash tables effectively function like a python dictionary the hash table itself is a 1dimensional array each index stores a list which contains the key value pairs as individual tuples the size of a hash table refers to how large the array is the size of any hash table is denoted by the letter m the load factor of a hash table is the number of inserted elements divided by the size of the hash table the load factor of any hash table is denoted by the greek letter lambda inserting into a hash table works by passing the key of a keyvalue pair into a hashing function the inner mechanism of the hash function is not very important but it typically returns the value mod the table size so that the output is a valid index in the table the keyvalue pair is then appended as a tuple to the end of the list stored at the index outputted by the hash function looking up a key in a hash table works by
value mod the table size so that the output is a valid index in the table the keyvalue pair is then appended as a tuple to the end of the list stored at the index outputted by the hash function looking up a key in a hash table works by passing the desired key through the hash function accessing all the tuples in the list stored at the corresponding index of the hash table and performing a linear search to match the searchedfor key with the key in each keyvalue pair we want to maintain a low load factor such that lambda is not greater than 09 if lambda becomes larger than 09 we should rehash the table which means using a larger hash table with more indices updating the hash function accordingly and passing all of the stored values through the new hash function and restoring them accordingly the relational data model has several benefits first it is for the most part the standard data model and query language used in industry second the model is acid compliant meaning that it adheres to the principles of atomicity consistency isolation and durability third the relational model works well with highly
relational data model has several benefits first it is for the most part the standard data model and query language used in industry second the model is acid compliant meaning that it adheres to the principles of atomicity consistency isolation and durability third the relational model works well with highly structured data fourth the relational model can handle large amounts of data finally the relational model is well understood with lots of tooling and lots of experience a transaction is a series of crud operations performed as a single unit of work crud stands for create read update delete and refer to the main operations you can perform on a database a transaction could be as simple as a select statement or more involved like joint update statements etc an entire transaction either succeeds commit or fails rollbackabort the four acid properties are atomicity consistency isolation and durability atomicity means that every transaction is treated as one unit the unit is either executed in full or no parts of it are executed consistency means that any transaction takes the database from one consistent state to another a consistent state is when all the data meets integrity conditions isolation means that
that every transaction is treated as one unit the unit is either executed in full or no parts of it are executed consistency means that any transaction takes the database from one consistent state to another a consistent state is when all the data meets integrity conditions isolation means that any two transactions can not affect each other if executed at the same time an issue with isolation could only happen if one transaction is reading from the data that another transaction is writing a dirty read is when the first transaction is able to read a row that has been written by the second transaction but has not yet been committed a nonrepeatable read is when a transaction returns different data than what existed when it started executing finally phantom reads occur when a transaction reads data twice and inserts both instances instead of just once durability means that once a transaction is completed and committed the changes are permanent shrek 5 is set to hit theaters on 12232026 to increase the capacity of a system you can scale either vertically or horizontally scaling vertically refers to adding more compute within one system scaling horizontally means adding more systems
once a transaction is completed and committed the changes are permanent shrek 5 is set to hit theaters on 12232026 to increase the capacity of a system you can scale either vertically or horizontally scaling vertically refers to adding more compute within one system scaling horizontally means adding more systems with the same compute a distributed system is a collection of independent computers that appear to its users as one computer the main characteristics of distributed systems is that they operate concurrently fail independently and do not have a global clock it is inevitable that distributed systems will need to have network partitioning which means that the systems need to be partition tolerant in other words if something happens to one node the system as a whole will be fine the cap theorem has three parts consistency availability and partition tolerance the theorem states that you can always have two of the three but never all three consistency means that you will always get the same result from the system availability means that you can always access the system partition tolerance means that the system will continue to operate despite network issues examples of products that include consistency and availability
of the three but never all three consistency means that you will always get the same result from the system availability means that you can always access the system partition tolerance means that the system will continue to operate despite network issues examples of products that include consistency and availability but not partition tolerance are relational database models like postgresql and mysql examples of products that include consistency and partition tolerance but not availability are mongo redis and hbase examples of products that include availability and partition tolerance but not consistency are couchdb cassandra and dynamodb nidhi foundations searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile or complex because you can nest the statement the baseline for efficiency is linear search start at the beginning of a list and proceed element by element until you find what youre looking for or you get to the last element and have not found it if there are n elements worst case scenario is going through all n values a record is a collection of values for attributes of a single entity instance a row of a table a
what youre looking for or you get to the last element and have not found it if there are n elements worst case scenario is going through all n values a record is a collection of values for attributes of a single entity instance a row of a table a collection is a set of records of the same entity type a table some collections are stored sequentially like a list search key is a value for an attribute from the entity type could be more than 1 attribute if each record takes up x bytes of memory then for n records we need nx bytes of memory there are 2 ways of storing the nx bytes in ram memory contiguously allocated list where all nx bytes are in a single chunk of memory and linked list where there are x 2 memory addresses that identify front versus back n for a linked list each record needs x bytes additional space for 1 or 2 memory addresses and individual records are linked together in a type of chain using memory addresses python technically does not have a true equivalent to a contiguous array arrays are fast for random access but
for a linked list each record needs x bytes additional space for 1 or 2 memory addresses and individual records are linked together in a type of chain using memory addresses python technically does not have a true equivalent to a contiguous array arrays are fast for random access but slow for random insertions anywhere but the end linked lists are slow for random access but fast for random insertions binary search keeps going to the middle iterativelyrecursively but it must be a sorted array the input is an array of values in sorted order target value output is the location index of where target is located or some value indicating target was not found the maximum number of searches log base 2 of n binary search only really applies to contiguously allocated list because it is super inefficient for linked lists best case is the target is found at middle 1 comparison inside the loop worst case is the target is not in the array log2 n comparisons or olog2n time complexity for linear search best case is the target is found at the first element only 1 comparison and worst case is the target is not in the
middle 1 comparison inside the loop worst case is the target is not in the array log2 n comparisons or olog2n time complexity for linear search best case is the target is found at the first element only 1 comparison and worst case is the target is not in the array n comparisons on time complexity lets say you have a table with two columns id and specialval assume data is stored on disk by the id columns value searching for a specific id would be fast but if we want to search for a specific specialval the only option is linear scan of that column we can not store data on disk sorted by both id and specialval at the same time because the data would have to be duplicated which is space inefficient therefore we need an external data structure to support faster searching by specialval than a linear scan one of your current options is an array of tuples specialval rownumber sorted by specialval and we could use binary search to quickly locate a particular specialval and find its corresponding row in the table but every insert into the table would be like inserting into a sorted
linear scan one of your current options is an array of tuples specialval rownumber sorted by specialval and we could use binary search to quickly locate a particular specialval and find its corresponding row in the table but every insert into the table would be like inserting into a sorted array which is slow another option is a linked list of tuples specialval rownumber sorted by specialval but searching for a specialval would be slow since a linear scan required but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search that would solve this issue is a binary search tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent extra notes about binary search and avl trees httpswwwcsrochestereduugildeacsc282slidesc12bstpdf ttpsicsucieduthorntonics46notesavltrees creatinginserting into a binary search tree conceptually lets say you want to insert 23 17 20 42 31 50 23 is the top node root 17 will descend to the left because it is smaller 20 will descend to the right of 17 43 will descend to the left from 23 31 will go
search tree conceptually lets say you want to insert 23 17 20 42 31 50 23 is the top node root 17 will descend to the left because it is smaller 20 will descend to the right of 17 43 will descend to the left from 23 31 will go left 50 will go right from 43 the level order traversal of this tree would be 23 17 43 20 31 50 tree traversal types are preorder post order in order and level order how when processing 23 root print 23 and then temporarily store 17 and 43 in an external data structure then print 17 and remove it from the external data structure and temporarily store 20 then remove 43 from the external data and print and temporarily store 31 and 50 then remove 20 and print no child so prints 31 then print 50 temporarily storing data happens in a queue and python has a special version called a deque binary search tree in python class binary tree nodeself value left right value integer left binary tree node right binary tree node root binarytreenode23 rootleft binarytreenode17 rootright binarytreenode43 how to get 20 rootleftright binarytreenode20 the order of values
in a queue and python has a special version called a deque binary search tree in python class binary tree nodeself value left right value integer left binary tree node right binary tree node root binarytreenode23 rootleft binarytreenode17 rootright binarytreenode43 how to get 20 rootleftright binarytreenode20 the order of values inserted into binary search tree matters changes the shape of the tree for example if the values are sorted it would be an unbalanced tree the goal is to minimize height of tree minimum height is always going to be from a complete tree all nodes filled except for last level an avl tree is an approximately balanced binary search tree it is self balancing and maintains a balance factor in each node at that node how balanced is the tree if that node were the root a tree is an avl tree if the absolute value of the height of left sub tree minus the height of right sub tree is less than or equal to 1 basically height of left sub tree and right sub tree can not differ by more than 1 inserting into an avl tree can balance or imbalance the tree but it would only
of left sub tree minus the height of right sub tree is less than or equal to 1 basically height of left sub tree and right sub tree can not differ by more than 1 inserting into an avl tree can balance or imbalance the tree but it would only change on the path from the most recently inserted node to the root once a tree becomes imbalanced there is an algorithm that will rebalance the tree alpha root where it is imbalanced node gets reorganized inserting can cause an imbalance in 4 ways left left insertion when you insert to the left subtree of the left child of the node of imbalance left right case when you insert into the right subtree of the left child of the node of imbalance right left case insert into the left subtree of the right child right right insert into the right subtree of the right child rebalancing rebalancing case 1 single rotation also applies to case 4 reassign cs left pointer to point to t2 and reassign as right pointer to point to c rebalancing case 2 double rotation also applies to case 3 need to separate t2 into its 2
subtree of the right child rebalancing rebalancing case 1 single rotation also applies to case 4 reassign cs left pointer to point to t2 and reassign as right pointer to point to c rebalancing case 2 double rotation also applies to case 3 need to separate t2 into its 2 children and basically do a single rotation twice how the reassignment of pointers happens case 4 rr aright points to the left child of c cleft points to a calculating height of a node is the maximum of the 2 heights of a nodes subtrees then you add one to the larger height memory a cpu has 16 or 32 registers depending on the processor registers are the closest to the cpu making them very fast but small and expensive the l1 cache is larger than registers but slower and cheaper per byte the l2 cache is even larger than l1 ram is the primary memory and operates at nanosecond speed secondary storage such as ssds and hdds is much slower measured in milliseconds ssds and hdds provide a lot of storage and are persistent meaning they can survive power cycles but they are extremely slow to improve speed database
l1 ram is the primary memory and operates at nanosecond speed secondary storage such as ssds and hdds is much slower measured in milliseconds ssds and hdds provide a lot of storage and are persistent meaning they can survive power cycles but they are extremely slow to improve speed database systems should minimize access to secondary storage a 64bit integer takes up 8 bytes of memory lets say there is an avl tree where each node has a key and a value and then a left and right pointers on a 64 bit machine this would take 4 8 32 bytes assuming an integer takes 8 bytes but if each node is stored in a different block each node would need 2048 bytes for an avl tree 144 log base 2 of n where n is the number of nodes in the tree is worst case lets say you have a sorted array of 128 integers worst case binary search on 128 integers is way faster than a single additional disk access even faster would be to do 3 children per node shallowerless tall tree this would minimize secondary storage disk accesses which is the most important thing for database
a sorted array of 128 integers worst case binary search on 128 integers is way faster than a single additional disk access even faster would be to do 3 children per node shallowerless tall tree this would minimize secondary storage disk accesses which is the most important thing for database systems b tree b tree is designed to maximize the number of values stored in each disk block each node has the maximum possible number of keys to reduce disk access a node with n1 keys has n children the b tree is optimized for diskbased indexing by minimizing disk access it is an mway tree with order m where m is the maximum number of keys in a node and m1 is the maximum number of children all nodes except the root must be at least half full but the root does not have this requirement insertions always happen at the leaf level and leaves are stored as a doubly linked list keys in nodes are kept sorted a b tree is shallower and wider than a bst or avl tree with the same number of nodes there are two types of nodes internal nodes which store keys and
at the leaf level and leaves are stored as a doubly linked list keys in nodes are kept sorted a b tree is shallower and wider than a bst or avl tree with the same number of nodes there are two types of nodes internal nodes which store keys and pointers to children and leaf nodes which store keys and data unlike btrees which are used for inmemory indexing b trees are designed for diskbased indexing a relational database management system rdbms increases efficiency through indexing direct storage control columnoriented or roworiented storage with columnoriented being faster for some large data processing materialized views query optimization caching prefetching precompiled stored procedures and data replication and partitioning to improve performance and data management column vs roworiented storage columnoriented storage and roworiented storage have different advantages depending on the use case transactions must be fully completed or not executed at all following a commit or rollbackabort process this ensures data integrity error recovery concurrency control reliable data storage and simplified error handling the relational model offers several benefits including a mostly standard data model and query language acid compliance atomicity consistency isolation durability strong support for highly structured data the ability to
a commit or rollbackabort process this ensures data integrity error recovery concurrency control reliable data storage and simplified error handling the relational model offers several benefits including a mostly standard data model and query language acid compliance atomicity consistency isolation durability strong support for highly structured data the ability to handle large amounts of data and widespread understanding with extensive tooling and experience acid acid ensures reliable database transactions through four key properties atomicity meaning a transaction must be fully completed or not executed at all all or none consistency transaction is always in a consistent state all data meets integrity constraints isolation preventing one transaction from negatively affecting another often managed through locking to avoid issues like dirty reads where a transaction reads uncommitted changes from another nonrepeatable reads where repeated queries within a transaction return different results due to committed changes by another transaction and phantom reads where rows are added or deleted by another transaction while the first is still running and durability guaranteeing that once a transaction is committed its changes are permanent even in the event of a system failure relational databases have downsides such as schemas changing over time not all apps needing full
rows are added or deleted by another transaction while the first is still running and durability guaranteeing that once a transaction is committed its changes are permanent even in the event of a system failure relational databases have downsides such as schemas changing over time not all apps needing full acid compliance joins being expensive and a lot of data being semistructured or unstructured like json or xml horizontal scaling can be hard and some apps need something more performant like realtime or lowlatency systems scaling conventional wisdom is to scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model why because scaling up is easier since there is no need to really modify your architecture but there are practical and financial limits to this however there are modern systems that make horizontal scaling less problematic one such solution is distributed systems a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems are that computers operate concurrently computers fail independently no shared global clock distributed storage has 2 directions replication and
less problematic one such solution is distributed systems a distributed system is a collection of independent computers that appear to its users as one computer andrew tennenbaum characteristics of distributed systems are that computers operate concurrently computers fail independently no shared global clock distributed storage has 2 directions replication and sharding replication is having the same data in multiple places sharding is having the data broken up into groups data is stored on more than 1 node typically replicated each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb is a new player on the scene many nosql systems support one or both models network partitioning is inevitable network failures system failures so overall system needs to be partition tolerant in other words the system can keep running even with network partition the cap theorem states it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency availability and partition tolerance consistency means that every read receives the most recent write or error thrown availability means that every request receives a nonerror response but no guarantee
it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency availability and partition tolerance consistency means that every read receives the most recent write or error thrown availability means that every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance means that the system can continue to operate despite arbitrary network issues cap theorem applied to a database think of it like a 3 way venn diagram consistency means that every user of the db has an identical view of the data at any given instant the definition of consistency in cap is different from that of acid availability means that in the event of a failure the database remains operational partition tolerance means the database can maintain operations in the event of the networks failing between two segments of the distributed system consistency and availability means the system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency and partition tolerance means if system responds with data from a distributed store it is always the latest
the distributed system consistency and availability means the system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency and partition tolerance means if system responds with data from a distributed store it is always the latest else data request is dropped availability and partition tolerance means the system always sends are responds based on distributed store but may not be the absolute latest data in reality cap theorem says if you can not limit the number of faults requests can be directed to any server and you insist on serving every request then you can not possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure nosql and key value databases acid transactions focus on data safety and are considered a pessimistic concurrency model because it assumes one transaction must protect itself from other transactions aka it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks the write lock analogy says that its like borrowing a book from
assumes one transaction must protect itself from other transactions aka it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks the write lock analogy says that its like borrowing a book from a library if you have it no one else can optimistic concurrency says that transactions do not obtain locks on data when they read or write it is considered optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok because you add last update timestamp and version number columns to every table and read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified low conflict systems backups analytical databases etc are read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems work by rolling back and rerunning transactions that encounter a conflict which is less efficient so a locking scheme pessimistic model might be
arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems work by rolling back and rerunning transactions that encounter a conflict which is less efficient so a locking scheme pessimistic model might be preferable nosql nosql first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is not only sql but sometimes thought of as nonrelational databases idea originally developed in part as a response to processing unstructured webbased data acid alternative for distributed systems is called base basically available guarantees the availability of the data per cap but response can be failureunreliable because the data is in an inconsistent or changing state system appears to work most of the time soft state says the state of the system could change over time even without input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency means the system will eventually become consistent and all writes will eventually stop so all nodesreplicas can be updated key value databases
over time even without input changes could be result of eventual consistency data stores dont have to be writeconsistent replicas dont have to be mutually consistent eventual consistency means the system will eventually become consistent and all writes will eventually stop so all nodesreplicas can be updated key value databases key value keyvalue stores are designed around simplicity speed and scalability simplicity means the data model is extremely simple comparatively tables in a rdbms are very complex simplicity lends itself to simple crud ops and api creation speed means it is usually deployed as inmemory db retrieving a value given its key is typically a o1 op because hash tables or similar data structs used under the hood there is no concept of complex queries or joins because they slow things down scalability means horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value data science use cases include storing intermediate results from data preprocessing and exploratory data analysis eda or experimenttesting ab results without using the production database using a feature store for fast retrieval of frequently accessed
environment the only guarantee is that all nodes will eventually converge on the same value data science use cases include storing intermediate results from data preprocessing and exploratory data analysis eda or experimenttesting ab results without using the production database using a feature store for fast retrieval of frequently accessed features for model training and prediction and model monitoring to store key performance metrics especially for realtime inference software engineering use cases include storing session information where all session data can be saved with a single put or post and retrieved quickly with a get user profiles and preferences where user settings like language time zone and ui preferences can be fetched with a single get shopping cart data which must be tied to the user and accessible across browsers machines and sessions and using a caching layer in front of a diskbased database for faster access connection pooling does a bunch of stuff basically makes it more efficient redis redis or remote directory server is an opensource inmemory database that supports durability by saving snapshots to disk at intervals or using an appendonly file to track changes for recovery after failures it is sometimes called a data structure store
a bunch of stuff basically makes it more efficient redis redis or remote directory server is an opensource inmemory database that supports durability by saving snapshots to disk at intervals or using an appendonly file to track changes for recovery after failures it is sometimes called a data structure store and is primarily a keyvalue store though it also supports models like graph spatial fulltext search vector and time series originally developed in 2009 in c redis is extremely fast handling over 100000 set operations per second and offers a rich collection of commands however it does not handle complex data lacks secondary indexes and only supports lookups by key in redis keys are typically strings but can be any binary sequence while values can be various data types including strings lists hashes sets sorted sets and geospatial data redis provides 16 default databases numbered 0 to 15 and is accessed through commands for setting and retrieving keyvalue pairs with many language libraries available the foundation data type is a string which maps one string to another and is commonly used for caching frequently accessed htmlcssjs storing config settings user info token management counting views or rate limiting the hash
accessed through commands for setting and retrieving keyvalue pairs with many language libraries available the foundation data type is a string which maps one string to another and is commonly used for caching frequently accessed htmlcssjs storing config settings user info token management counting views or rate limiting the hash type stores keyvalue entries as fieldvalue pairs and is useful for representing objects session management userevent tracking and active session tracking the list type is a linked list of string values commonly used for stacks and queues queue management logging social media feeds chat message history and batch processing linked lists allow efficient o1 insertion at the front or end the json type supports full json syntax and is stored in a binary treestructure for fast access the set type is an unordered collection of unique strings useful for tracking unique items eg ip addresses primitive relations eg students in a course access control lists and social network connections redispy is the standard client for python maintained by redis itself why redis over mysqls3 or other relational database because all data is stored in disk so it is faster latency issues much faster than select statements redis pipelines help avoid
a course access control lists and social network connections redispy is the standard client for python maintained by redis itself why redis over mysqls3 or other relational database because all data is stored in disk so it is faster latency issues much faster than select statements redis pipelines help avoid multiple related calls to the server which needs less network overhead document databases and mongodb document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable json javascript object notation is a lightweight datainterchange format easy for humans to read and write easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array and an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages which makes json a great data interchange format bson is binary json it is a binaryencoded serialization of a jsonlike document structure it supports extended types
languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages which makes json a great data interchange format bson is binary json it is a binaryencoded serialization of a jsonlike document structure it supports extended types not part of basic json eg date binarydata etc it is lightweight and keeps space overhead to a minimum it is traversable which means its designed to be easily traversed which is vitally important to a document db it is efficient because encoding and decoding must be efficient it is also supported by many modern programming languages xml or extensible markup language is the precursor to json as data exchange format xml and css are used together for web pages content and formatting xml is structurally like html but tag set is extensible some toolstechnologies used with xml are xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document and xslt extensible stylesheet language transformation a tool to transform xml into other
a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document and xslt extensible stylesheet language transformation a tool to transform xml into other formats including nonxml formats such as html why document databases they address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer mongodb mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving over 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 and provided documentdb as a service no predefined schema for documents is needed every document in a collection could have different dataschema rich query support provides robust support for all crud ops indexing supports primary and secondary indices on
mongodb was short for humongous database mongodb atlas released in 2016 and provided documentdb as a service no predefined schema for documents is needed every document in a collection could have different dataschema rich query support provides robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover and load balancing is built in mongodb atlas is a fully managed mongodb service in the cloud dbaas mongodb enterprise is a subscriptionbased selfmanaged version of mongodb mongodb community is sourceavailable freetouse selfmanaged relational database versus mongo a database in a relational database is called a database in mongodb a table or view in a relational database is called a collection in mongodb a row in a relational database is called a document in mongodb a column in a relational database is called a field in mongodb an index in a relational database is called an index in mongodb a join in a relational database is called an embedded document in mongodb a foreign key in a relational database is called a reference in mongodb interacting with mongodb is done using the following mongosh mongodb shell is a cli tool for
relational database is called an index in mongodb a join in a relational database is called an embedded document in mongodb a foreign key in a relational database is called a reference in mongodb interacting with mongodb is done using the following mongosh mongodb shell is a cli tool for interacting with a mongodb instance mongodb compass is a free opensource gui to work with a mongodb database datagrip and other 3rd party tools can also be used every major language has a library to interface with mongodb such as pymongo python mongoose javascriptnode and more pymongo is a python library for interfacing with mongodb instances intro to graph data model graph database is made up of nodes connected by edges which can be directed or undirected each edge and node is uniquely identifiable you can also add extra properties via key value pairs queries are based on the structure of database called a cipher examples of graph data model are social networks web pages chemical and biological data the underlying thing that links pages of the internet which is a is big graph is called https 404 error message a labelled property graph has edgesarcsrelationships and nodesvertices this is
of database called a cipher examples of graph data model are social networks web pages chemical and biological data the underlying thing that links pages of the internet which is a is big graph is called https 404 error message a labelled property graph has edgesarcsrelationships and nodesvertices this is the type that has properties with keys and values labels are used to mark a node as part of a group nodes with no relationships are allowed but edges not connected to nodes are not a path is getting from one node to another in order with no repeating nodes or edges it does not matter if you count nodes or edges in a graph algorithm as long as it is consistent there are connected vs disconnected graphs connected means there is a path between any two nodes in the graph weighted vs unweighted means the edge has a weight property important for some algorithms directed vs undirected means relationships edges define a start and end node directed graphs can be a loop with arrows going both ways acyclic vs cyclic means the graph contains no cycles spare vs dense has to do with the number of edges compared to
property important for some algorithms directed vs undirected means relationships edges define a start and end node directed graphs can be a loop with arrows going both ways acyclic vs cyclic means the graph contains no cycles spare vs dense has to do with the number of edges compared to number of nodes trees have no cycles pathfinding has to do with finding the shortest path which can mean fewest edges or lowest weight when summed average shortest path can be used to monitor efficiency and resiliency of a network pagerank search engines used to use the number of links to a website as the first search result then it became how many pages link to the pages that link to the most page breadth first search visit nearest neighbors first is different from depth first search walks down each branch first centrality is determining which nodes are more important in a network compared to other nodes degree closeness betweenness pagerank community detection is evaluating clustering or partitioning of nodes of a graph and tendency to strengthen or break apart famous graph algorithms include dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with
in a network compared to other nodes degree closeness betweenness pagerank community detection is evaluating clustering or partitioning of nodes of a graph and tendency to strengthen or break apart famous graph algorithms include dijkstras algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstras with added feature of using a heuristic to guide traversal and pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j neo4j is a nosql database that supports both transactional and analytical processing of graphbased data it is relatively new schemaoptional acidcompliant and supports various types of indexing and distributed computing similar databases include microsoft cosmosdb and amazon neptune neo4j uses cypher a graph query language created in 2011 and supports plugins like apoc for additional functions and the graph data science plugin for running graph algorithms docker compose is a tool for managing multicontainer applications using a yaml configuration file it allows starting stopping and scaling services with a single command ensuring consistent environments interaction is mostly through the command line and env files help manage environment variables across platforms in neo4j
for running graph algorithms docker compose is a tool for managing multicontainer applications using a yaml configuration file it allows starting stopping and scaling services with a single command ensuring consistent environments interaction is mostly through the command line and env files help manage environment variables across platforms in neo4j relationships are directed docker compose docker compose supports multicontainer management with a declarative setup using a yaml file dockercomposeyaml to define services volumes and networks a single command can start stop or scale multiple services at once ensuring a consistent environment interaction is mostly through the command line and env files store environment variables to keep settings separate across platforms envlocal envdev envprod port numbers maximum port number is 65535 range of ports for system services need root access is 01023 http 80 https 443 ssh 22 ftp 21 retrieval augmented generation rag the outputs are contextual to the input in other words the pdf of the class notes is contextual elements and augmenting what is already in the model with that context ansley searching in database systems searching is a common operation in database systems in sql the select statement is one of the most versatile and complex operations
in other words the pdf of the class notes is contextual elements and augmenting what is already in the model with that context ansley searching in database systems searching is a common operation in database systems in sql the select statement is one of the most versatile and complex operations baseline efficiency linear search linear search starts at the beginning of a list and proceeds element by element until 1 the target is found 2 the last element is reached without finding the target key terms record a collection of attribute values for a single entity instance a row in a table collection a set of records of the same entity type a table search key a value from an attribute used to search it can be a single or multiple attributes lists of records memory usage if each record takes x bytes for n records the total memory required is n x bytes contiguously allocated list all n x bytes are allocated as a single memory block linked list each record takes x bytes plus additional memory for one or two addresses to link records together contiguous vs linked lists contiguous allocated list array memory is allocated as a
is n x bytes contiguously allocated list all n x bytes are allocated as a single memory block linked list each record takes x bytes plus additional memory for one or two addresses to link records together contiguous vs linked lists contiguous allocated list array memory is allocated as a single block faster for random access slower for insertions except at the end linked list records are linked by memory addresses extra storage is needed for the address faster for insertions anywhere in the list slower for random access insertion examples array inserting after the second record requires moving 5 records to make space linked list inserting after the second record does not require moving other records observations arrays fast for random access but slow for insertions linked lists slow for random access but fast for insertions binary search input a sorted array and a target value output the index of the target or an indication that it is not found def binarysearcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 example in a sorted array the target
of the target or an indication that it is not found def binarysearcharr target left right 0 lenarr 1 while left right mid left right 2 if arrmid target return mid elif arrmid target left mid 1 else right mid 1 return 1 example in a sorted array the target a is located by adjusting left or right based on the midpoint comparison time complexity linear search best case o1 target found at the first element worst case on target is not found binary search best case o1 target found at midpoint worst case olog2 n target not found searching in databases storing by id fast for searching a specific id searching by special value linear scan is required which is inefficient issue storing data by both id and special value it is not possible to store data sorted by both id and special value at the same time due to space inefficiency solution external data structures option 1 array of tuples specialval rownumber sorted by specialval binary search can quickly find a specialval but inserting is slow because the array is sorted option 2 linked list of tuples specialval rownumber sorted by specialval insertion is fast but searching requires
to space inefficiency solution external data structures option 1 array of tuples specialval rownumber sorted by specialval binary search can quickly find a specialval but inserting is slow because the array is sorted option 2 linked list of tuples specialval rownumber sorted by specialval insertion is fast but searching requires a linear scan what about fast insert and search a binary search tree bst can provide both fast insertion and searching in a bst each node in the left subtree is less than the parent node each node in the right subtree is greater than the parent node relational databases distributed systems benefits of the relational model mostly standard data model and query language acid compliance works well with highly structured data can handle large amounts of data well understood extensive tooling and expertise relational database performance enhancements indexing direct storage control columnoriented vs roworiented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing a transaction is a sequence of one or more crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort benefits data integrity error recovery concurrency control
views precompiled stored procedures data replication and partitioning transaction processing a transaction is a sequence of one or more crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort benefits data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit either fully executed or no parts are executed consistency a transaction takes a database from one consistent state to another consistent state all data meets integrity constraints isolation when two transactions t1 and t2 execute simultaneously they should not affect each other if both t1 and t2 are reading data no problem if t1 reads data that t2 may be writing potential issues include dirty read t1 reads a row modified by uncommitted t2 nonrepeatable read t1 executes same query twice but gets different values because t2 committed changes phantom reads t1 is running while t2 addsdeletes rows from the set t1 is using durability once a transaction completes and commits successfully changes are permanent changes are preserved even during system failure example transaction money transfer sqlcopydelimiter create procedure transfer in senderid int in
values because t2 committed changes phantom reads t1 is running while t2 addsdeletes rows from the set t1 is using durability once a transaction completes and commits successfully changes are permanent changes are preserved even during system failure example transaction money transfer sqlcopydelimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from sender update accounts set balance balance amount where accountid senderid attempt to credit money to receiver update accounts set balance balance amount where accountid receiverid check if there are sufficient funds in sender account if select balance from accounts where accountid senderid 0 then roll back the transaction if insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter limitations of relational databases schemas evolve over time not all applications need full acid compliance join
sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter limitations of relational databases schemas evolve over time not all applications need full acid compliance join operations can be expensive semistructured or unstructured data json xml not handled optimally horizontal scaling challenges performance limitations for realtime lowlatency systems scalability vertical vs horizontal conventional wisdom scale vertically up with bigger more powerful systems only scale horizontally out when highavailability necessitates distributed computing considerations vertical scaling is simpler no architecture changes but has practical and financial limits modern systems make horizontal scaling more manageable distributed systems a distributed system is a collection of independent computers that appear to its users as one computer andrew tanenbaum characteristics computers operate concurrently computers fail independently no shared global clock distributed storage approaches distributed data stores data stored on multiple nodes typically replicated each block of data is available on n nodes can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb is a newer distributed sql database many nosql systems support distribution models important consideration network partitioning is
approaches distributed data stores data stored on multiple nodes typically replicated each block of data is available on n nodes can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb is a newer distributed sql database many nosql systems support distribution models important consideration network partitioning is inevitable network failures system failures will occur overall system needs to be partition tolerant continue running despite network partitions the cap theorem the cap theorem states that a distributed data store can not simultaneously provide more than two of these three guarantees consistency every read receives most recent write or an error availability every request receives a nonerror response but not guaranteed to contain most recent write partition tolerance system operates despite arbitrary network issues database view of cap tradeoffs consistency availability system always responds with latest data every request gets a response may not handle network partitions well consistency partition tolerance if system responds with data its always the latest otherwise data request is dropped sacrifices some availability availability partition tolerance system always responds may not provide absolute latest data most common choice for distributed systems cap in reality what it really means if you can not limit fault
partition tolerance if system responds with data its always the latest otherwise data request is dropped sacrifices some availability availability partition tolerance system always responds may not provide absolute latest data most common choice for distributed systems cap in reality what it really means if you can not limit fault numbers requests can go to any server and you insist on serving every request then consistency is impossible common interpretation you must always sacrifice one consistency availability or partition tolerance nosql keyvalue databases distributed databases and concurrency models pessimistic concurrency acid focuses on data safety assumes transactions need protection from other transactions conflicts prevented by locking resources until transaction completion uses both read and write locks analogy borrowing a library book if you have it no one else can use it more details how databases guarantee isolation optimistic concurrency transactions do not obtain locks on data for readingwriting assumes conflicts are unlikely to occur implementation add timestamp and version columns to tables read these when changing data check at transaction end if another transaction modified them works well for lowconflict systems backups analytical dbs readheavy systems systems that can tolerate rollbacks and retries less efficient for highconflict systems where locking
are unlikely to occur implementation add timestamp and version columns to tables read these when changing data check at transaction end if another transaction modified them works well for lowconflict systems backups analytical dbs readheavy systems systems that can tolerate rollbacks and retries less efficient for highconflict systems where locking may be preferable nosql overview term nosql first used in 1998 by carlo strozzi for a relational database without sql modern meaning not only sql sometimes interpreted as nonrelational dbs developed partly as a response to processing unstructured webbased data brief history of nonrelational databases cap theorem you can have 2 but not all 3 of the following consistency every user has an identical view of data at any given instant availability database remains operational during failures partition tolerance database maintains operations despite network failures between system segments note consistency in cap differs from consistency in acid cap tradeoffs consistency availability system always responds with latest data but may not handle network partitions consistency partition tolerance system responds with latest data or drops the request availability partition tolerance system always responds but may not provide the absolute latest data base model acid alternative for distributed systems basically available data is
availability system always responds with latest data but may not handle network partitions consistency partition tolerance system responds with latest data or drops the request availability partition tolerance system always responds but may not provide the absolute latest data base model acid alternative for distributed systems basically available data is available but might be inconsistent or changing soft state system state may change without input due to eventual consistency data stores dont require writeconsistency replicas dont require mutual consistency eventual consistency system will eventually become consistent when writes stop keyvalue databases core design principles simplicity extremely simple data model key value supports basic crud operations and api creation much simpler than relational tables speed typically deployed as inmemory databases o1 retrieval operations using hash tables or similar structures no complex queries or joins to slow things down scalability easy horizontal scaling by adding nodes uses eventual consistency in distributed environments use cases data science applications edaexperimentation results store intermediate results from data preprocessing ab testing results without production db impact feature store lowlatency retrieval for model training and prediction model monitoring store realtime performance metrics software engineering applications session information storage fast singleoperation retrieval and storage user profiles preferences language
use cases data science applications edaexperimentation results store intermediate results from data preprocessing ab testing results without production db impact feature store lowlatency retrieval for model training and prediction model monitoring store realtime performance metrics software engineering applications session information storage fast singleoperation retrieval and storage user profiles preferences language timezone ui preferences shopping cart data crossbrowserdevice availability caching layer frontend for diskbased databases redis remote directory server overview open source inmemory database sometimes called a data structure store primarily a kv store but supports other models graph spatial full text search vector time series topranked keyvalue store according to dbenginescom key features inmemory but supports data durability through disk snapshots at intervals appendonly file journal for rollforward recovery originally developed in 2009 in c performance 100000 set operationssecond rich command set limitations no complex data no secondary indexes lookup by key only data types keys usually strings can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data json redis databases 16 databases by default numbered 015 no other naming conventions interaction through commands or language libraries redis data types and commands strings simplest data type maps string
any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string string geospatial data json redis databases 16 databases by default numbered 015 no other naming conventions interaction through commands or language libraries redis data types and commands strings simplest data type maps string to string can store text serialized objects binary arrays use cases caching htmlcssjs fragments configurationuser settings token management page view counting rate limiting string commands copyset pathtoresource 0 set user1 john doe get pathtoresource exists user1 del user1 keys user select 5 select database 5 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 setnx key value set only if key doesnt exist hashes value is a collection of fieldvalue pairs use cases represent basic objectsstructures session information management userevent tracking active session tracking hash commands copyhset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 lists value is a linked list of string values use cases stacks and queues implementation message passing queues logging systems social media streamsfeeds chat application message
bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight hincrby bike1 price 100 lists value is a linked list of string values use cases stacks and queues implementation message passing queues logging systems social media streamsfeeds chat application message history batch processing task queues list commands copy queue operations lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs stack operations lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs other operations llen mylist lrange mylist 0 3 elements from index 0 to 3 lrange mylist 0 0 first element lrange mylist 2 1 last two elements sets unordered collection of unique strings use cases track unique items ip addresses primitive relations access control lists social network friendsgroup membership supports set operations set commands copysadd ds4300 mark sadd ds4300 sam sadd cs3200 nick sadd cs3200 sam sismember ds4300 mark check membership scard ds4300 count elements sinter ds4300 cs3200 intersection sdiff ds4300 cs3200 difference srem ds4300 mark remove element srandmember ds4300 random member json type full support of json standard uses jsonpath syntax for parsingnavigating stored internally in binary tree structure for fast subelement access redis setup guide prerequisites docker desktop installed jetbrains
ds4300 count elements sinter ds4300 cs3200 intersection sdiff ds4300 cs3200 difference srem ds4300 mark remove element srandmember ds4300 random member json type full support of json standard uses jsonpath syntax for parsingnavigating stored internally in binary tree structure for fast subelement access redis setup guide prerequisites docker desktop installed jetbrains datagrip installed step 1 find the redis image open docker desktop use the builtin search to find the redis image click run step 2 configure run the container give the new container a name enter 6379 in the host port field standard redis port click run allow docker time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source by either clicking the icon in the database explorer selecting new from the file menu step 4 configure the data source give the data source a name install drivers if needed a message will appear above test connection if required test the connection to redis click ok if connection test was successful notes redis can store various data types strings numbers json objects binary objects etc if drivers arent already installed datagrip will show a message above the test connection
needed a message will appear above test connection if required test the connection to redis click ok if connection test was successful notes redis can store various data types strings numbers json objects binary objects etc if drivers arent already installed datagrip will show a message above the test connection button redispy guide overview redispy is the official python client for redis maintained by redis inc github redisredispy installation pip install redis storage capability handles strings numbers json objects and binary data connection pythoncopyimport redis connect to redis server redisclient redisredis hostlocalhost for docker localhost or 127001 port6379 default redis port db2 database number 015 decoderesponsestrue converts byte responses to strings key commands by data type string operations pythoncopy basic operations redisclientsetclickcountabc 0 value redisclientgetclickcountabc redisclientincrclickcountabc multiple operations redisclientmsetkey1 val1 key2 val2 key3 val3 values redisclientmgetkey1 key2 key3 returns val1 val2 val3 available string commands write set mset setex msetnx setnx read get mget getex getdel numeric incr decr incrby decrby text strlen append list operations pythoncopy create and populate list redisclientrpushnames mark sam nick allnames redisclientlrangenames 0 1 returns mark sam nick available list commands left operations lpush lpop right operations rpush rpop management lset lrem lrange llen lpos
setnx read get mget getex getdel numeric incr decr incrby decrby text strlen append list operations pythoncopy create and populate list redisclientrpushnames mark sam nick allnames redisclientlrangenames 0 1 returns mark sam nick available list commands left operations lpush lpop right operations rpush rpop management lset lrem lrange llen lpos hash operations pythoncopy create and populate hash redisclienthsetusersession123 mapping first sam last uelle company redis age 30 get all hash fields userdata redisclienthgetallusersession123 available hash commands basic hset hget hgetall management hkeys hdel hexists hlen hstrlen pipelines reduce network overhead by batching multiple commands pythoncopy create pipeline pipe redisclientpipeline set multiple values in one execution for i in range5 pipesetfseati fi results pipeexecute true true true true true chain commands results redisclientpipelinegetseat0getseat3getseat4execute results 0 3 4 redis in mldata science redis serves as an essential component in ml architectures feature stores vector databases model serving caching layers realtime processing resources full redis command list redispy documentation document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable what is json json javascript object notation is a lightweight datainterchange format that is easy for humans
full redis command list redispy documentation document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable what is json json javascript object notation is a lightweight datainterchange format that is easy for humans to read and write and easy for machines to parse and generate json is built on two structures 1 a collection of namevalue pairs eg object record dictionary hash table associative array 2 an ordered list of values eg array vector list sequence these two structures are supported by almost all modern programming languages making json an ideal data interchange format json syntax binary json bson bson binary json is a binaryencoded serialization of a jsonlike document structure supports extended types like date and binarydata lightweight to minimize space overhead designed to be easily traversed which is important for document databases efficient encoding and decoding supported by many programming languages xml extensible markup language xml was the precursor to json as a data exchange format xml css is used to create web pages that separate content and formatting xml is structurally similar to html but the tag set is extensible
databases efficient encoding and decoding supported by many programming languages xml extensible markup language xml was the precursor to json as a data exchange format xml css is used to create web pages that separate content and formatting xml is structurally similar to html but the tag set is extensible xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml document xquery a query language for xml documents similar to sql dtd a language to define the structure of an xml document xslt a tool to transform xml into other formats including html why document databases document databases solve the impedance mismatch problem between objectoriented programming oop and how relational databases structure data in oop inheritance and composition of types are used however saving complex objects in a relational database requires deconstructing them document databases allow the structure of a document to be selfdescribing making them a natural fit for applications that use jsonxml for data transport mongodb mongodb started in 2007 after doubleclick was acquired by google and three of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas was released in 2016 offering
fit for applications that use jsonxml for data transport mongodb mongodb started in 2007 after doubleclick was acquired by google and three of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas was released in 2016 offering document databases as a service mongodb structure database collection a collection b collection c document 1 document 2 document 3 mongodb documents no predefined schema for documents every document in a collection can have a different schema relational vs mongodocument db rdbms vs mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support full support for crud operations indexing supports primary and secondary indexes on document fields replication supports replica sets with automatic failover load balancing builtin load balancing mongodb versions mongodb atlas fully managed mongodb service dbaas mongodb enterprise subscriptionbased selfmanaged version mongodb community sourceavailable freetouse selfmanaged version interacting with mongodb mongosh mongodb shell cli tool mongodb compass free opensource gui for working with mongodb datagrip and other 3rdparty tools libraries to interface with mongodb for various languages eg pymongo for python mongoose for javascript mongodb community
mongodb enterprise subscriptionbased selfmanaged version mongodb community sourceavailable freetouse selfmanaged version interacting with mongodb mongosh mongodb shell cli tool mongodb compass free opensource gui for working with mongodb datagrip and other 3rdparty tools libraries to interface with mongodb for various languages eg pymongo for python mongoose for javascript mongodb community edition in docker create a container and map the hostcontainer port 27017 provide an initial username and password for the superuser mongodb compass gui tool for interacting with mongodb download and install from the official website load mflix sample data set 1 create a new database called mflix 2 download and unzip the mflix sample dataset 3 import json files for users theaters movies and comments into new collections in the mflix database creating a database and collection use mongosh to interact with mongodb find queries in mongodb are similar to sql select statements example to select all users dbusersfind to filter users by name dbusersfindname davos seaworth to filter movies released in 2010 and having a specific rating or genre dbmoviesfindyear 2010 or awardswins gte 5 genres drama count documents in a collection to count documents use countdocuments method example to count movies from 2010 with at least 5
to filter users by name dbusersfindname davos seaworth to filter movies released in 2010 and having a specific rating or genre dbmoviesfindyear 2010 or awardswins gte 5 genres drama count documents in a collection to count documents use countdocuments method example to count movies from 2010 with at least 5 awards or with a drama genre dbmoviescountdocumentsyear 2010 or awardswins gte 5 genres drama pymongo pymongo is a python library for interfacing with mongodb instances pymongo pymongo is a python library for interfacing with mongodb instances getting a database and collection from pymongo import mongoclient client mongoclient mongodbusernamepwlocalhost27017 db clientds4300 or clientds4300 collection dbmycollection or dbmycollection inserting a single document db clientds4300 collection dbmycollection post author mark text mongodb is cool tags mongodb python postid collectioninsertonepostinsertedid printpostid find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in your python environment with pip install jupyterlab download and unzip this zip file it contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run
time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in your python environment with pip install jupyterlab download and unzip this zip file it contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab graph databases and graph theory what is a graph database data model based on the graph data structure composed of nodes vertices and edges relationships edges connect nodes each element is uniquely identified each can contain properties eg name occupation supports graphoriented operations traversals shortest path algorithms many other specialized queries realworld graph applications social networks social media platforms instagram facebook modeling social interactions in psychology and sociology the web a large graph of pages nodes connected by hyperlinks edges chemical and biological data systems biology genetics chemical interaction relationships labeled property graph model composed of node vertex objects and relationship edge objects labels used to mark nodes as part of a group properties keyvalue pair attributes that can exist on both nodes and relationships structure rules nodes with no relationships are permitted edges not connected to nodes are not allowed graph example components 2 labels person car
objects and relationship edge objects labels used to mark nodes as part of a group properties keyvalue pair attributes that can exist on both nodes and relationships structure rules nodes with no relationships are permitted edges not connected to nodes are not allowed graph example components 2 labels person car 4 relationship types drives owns liveswith marriedto properties attributes attached to nodes and relationships paths in graphs a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated example valid path 1 2 6 5 invalid path 1 2 6 2 3 node 2 repeats graph types and properties connected vs disconnected connected a path exists between any two nodes in the graph disconnected contains isolated components with no paths between them weighted vs unweighted weighted edges have a weight property important for certain algorithms unweighted no weights assigned to edges directed vs undirected directed relationships edges define a specific start and end node undirected relationships have no direction cyclic vs acyclic cyclic graph contains at least one cycle acyclic graph contains no cycles sparse vs dense sparse relatively few edges compared to the maximum possible dense has many edges approaching the
vs undirected directed relationships edges define a specific start and end node undirected relationships have no direction cyclic vs acyclic cyclic graph contains at least one cycle acyclic graph contains no cycles sparse vs dense sparse relatively few edges compared to the maximum possible dense has many edges approaching the maximum possible number trees a tree is a special type of graph connected acyclic undirected each node except the root has exactly one parent graph algorithms overview pathfinding algorithms definition find the shortest path between nodes fewest edges or lowest weight use case monitor network efficiency and resiliency using average shortest path variants minimum spanning tree cycle detection maxmin flow algorithms search approaches bfs breadthfirst search explores all neighbors before moving deeper dfs depthfirst search explores as far as possible along branches before backtracking centrality community detection centrality identifies important nodes in a network eg social network influencers community detection evaluates clusteringpartitioning of nodes and their cohesion famous graph algorithms dijkstras algorithm singlesource shortest path for positively weighted graphs a algorithm enhanced dijkstras that uses heuristics to guide traversal pagerank measures node importance based on incoming relationships and their sources neo4j type graph database system supporting transactional and analytical processing
evaluates clusteringpartitioning of nodes and their cohesion famous graph algorithms dijkstras algorithm singlesource shortest path for positively weighted graphs a algorithm enhanced dijkstras that uses heuristics to guide traversal pagerank measures node importance based on incoming relationships and their sources neo4j type graph database system supporting transactional and analytical processing classification nosql database with schemaoptional design features various indexing capabilities acid compliance distributed computing support similar systems microsoft cosmos db amazon neptune this optimized format eliminates the unnecessary numbering 1522 organizes content into logical sections and uses consistent formatting that llms can easily process and understand cells celltype markdown id 02ead21fdc3b4536aa57f2ead5395348 metadata source mongodb aggregation examples celltype markdown id 66da7d8d4c4f4c608e22d0c023c35a0f metadata source ensure you have pymongo installed before running cells in this notebook celltype code executioncount null id 5e29baf989a046189f912c8087128b7d metadata outputs source import pymongon from bsonjsonutil import dumpsn import pprintn n update the uri with your username and password n n uri mongodbmarkabc123localhost27017n client pymongomongoclienturin mflixdb clientmflixn demodb clientdemodb celltype markdown id 1565e7882c724ab6a5b336e05334233b metadata source about aggregates in pymongon n aggregation uses pipelinesn a pipeline is a sequence of stages through which documents proceedn some of the different stages that can be used aren matchn projectn sortn limitn unwindn groupn
n uri mongodbmarkabc123localhost27017n client pymongomongoclienturin mflixdb clientmflixn demodb clientdemodb celltype markdown id 1565e7882c724ab6a5b336e05334233b metadata source about aggregates in pymongon n aggregation uses pipelinesn a pipeline is a sequence of stages through which documents proceedn some of the different stages that can be used aren matchn projectn sortn limitn unwindn groupn lookup celltype markdown id 921429f53125475eb0afa871b9a0799e metadata source match celltype code executioncount null id 1a9f528f5ffa40fa99b65a3bc36b3e33 metadata scrolled true outputs source c mflixdbmoviesaggregaten match year lte 1920n n n printdumpsc indent4 celltype markdown id e180713d92ec4392ba996c5a96775903 metadata source match and project celltype code executioncount null id cc479ecc724149d79cc25a57cd40bb5c metadata scrolled true outputs source c mflixdbmoviesaggregaten match year lte 1920n project id0 title 1 cast 1n n n printdumpsc indent4 celltype markdown id 7f7d7755e9fa4061ac985f31b75201f2 metadata source match project limit and sort celltype code executioncount null id d7e42c04668e4dd6abf77a081c126c13 metadata scrolled true outputs source c mflixdbmoviesaggregaten match year lte 1920n sort title 1n limit 5n project id0 title 1 cast 1n n n printdumpsc indent4 celltype markdown id 6e58c4a2ff4248a18493aba643b7592d metadata source unwind celltype code executioncount null id 02ac7ecc69ee45cb83efb271afaccbe1 metadata scrolled true outputs source c mflixdbmoviesaggregaten match year lte 1920n sort imdbrating 1n limit 5n unwind castn project id0 title 1 cast 1 rating imdbratingn n n printdumpsc
title 1 cast 1n n n printdumpsc indent4 celltype markdown id 6e58c4a2ff4248a18493aba643b7592d metadata source unwind celltype code executioncount null id 02ac7ecc69ee45cb83efb271afaccbe1 metadata scrolled true outputs source c mflixdbmoviesaggregaten match year lte 1920n sort imdbrating 1n limit 5n unwind castn project id0 title 1 cast 1 rating imdbratingn n n printdumpsc indent4 celltype markdown id 4a2edddc422e4f678ddb87343c5eb004 metadata source grouping celltype code executioncount null id 4e3b2ef1681c4cf4baba7d1399de1dd1 metadata scrolled true outputs source what is the average imdb rating of all movies by year sort the data by yearn n c mflixdbmoviesaggregaten group id release year year avg rating avg imdbratingn sort id 1n n n printdumpsc indent 2 celltype code executioncount null id 665047a187cb422f91ae645714020a32 metadata scrolled true outputs source what is the average imdb rating of all movies by year sort the data by avg rating in decreasing ordern n c mflixdbmoviesaggregaten group id release year year avg rating avg imdbratingn sort avg rating 1 id 1n n n printdumpsc indent 2 celltype markdown id f3bb33037cce44f2a9bb7503ee2741e1 metadata source lookup celltype code executioncount null id 6f869bbe701b4fb3980fbe9f1879b1ed metadata scrolled true outputs source data demodbcustomersaggregaten n lookup n from ordersn localfield custidn foreignfield custidn as ordersn n n project id 0 address 0n n printdumpsdata indent
rating 1 id 1n n n printdumpsc indent 2 celltype markdown id f3bb33037cce44f2a9bb7503ee2741e1 metadata source lookup celltype code executioncount null id 6f869bbe701b4fb3980fbe9f1879b1ed metadata scrolled true outputs source data demodbcustomersaggregaten n lookup n from ordersn localfield custidn foreignfield custidn as ordersn n n project id 0 address 0n n printdumpsdata indent 2 celltype markdown id 7e24db4a38f04464ab8b19d675af29db metadata source reformatting queries celltype code executioncount null id 1d7f801c05e14c05baad673c36302d20 metadata outputs source match match year lte 1920n limit limit 5n project project id0 title 1 cast 1 rating imdbratingn n agg mflixdbmoviesaggregatematch limit projectn printdumpsagg indent2 celltype code executioncount null id 76e9276d938d45e392f4f2daa42ce704 metadata outputs source metadata kernelspec displayname python 3 ipykernel language python name python3 languageinfo codemirrormode name ipython version 3 fileextension py mimetype textxpython name python nbconvertexporter python pygmentslexer ipython3 version 3119 nbformat 4 nbformatminor 5 cells celltype markdown id 02ead21fdc3b4536aa57f2ead5395348 metadata source mongodb pymongo example queries celltype markdown id 66da7d8d4c4f4c608e22d0c023c35a0f metadata source make sure your mongodb container is runningn make sure you have pymongo installed before running cells in this notebook if not use pip install pymongo celltype code executioncount null id 5e29baf989a046189f912c8087128b7d metadata outputs source import pymongon from bsonjsonutil import dumpsn n update the uri with your username and password n
make sure your mongodb container is runningn make sure you have pymongo installed before running cells in this notebook if not use pip install pymongo celltype code executioncount null id 5e29baf989a046189f912c8087128b7d metadata outputs source import pymongon from bsonjsonutil import dumpsn n update the uri with your username and password n n uri mongodbusernamepasswordlocalhost27017n client pymongomongoclienturin mflixdb clientmflix celltype code executioncount null id e4ec02e07f9240dca6dbc0bd0e2d5b32 metadata outputs source setup demodb with 2 collectionsn demodbcustomersdropn demodbordersdropn n customers n custid c13 name t cruise address street 201 main st city st louis mo zipcode 63101 rating 750 n custid c25 name m streep address street 690 river st city hanover ma zipcode 02340 rating 690 n custid c31 name b pitt address street 360 mountain ave city st louis mo zipcode 63101 n custid c35 name j roberts address street 420 green st city boston ma zipcode 02115 rating 565 n custid c37 name t hanks address street 120 harbor blvd city boston ma zipcode 02115 rating 750 n custid c41 name r duvall address street 150 market st city st louis mo zipcode 63101 rating 640 n custid c47 name s loren address street via del corso city rome italy rating 625
name t hanks address street 120 harbor blvd city boston ma zipcode 02115 rating 750 n custid c41 name r duvall address street 150 market st city st louis mo zipcode 63101 rating 640 n custid c47 name s loren address street via del corso city rome italy rating 625 n n n orders n orderno 1001 custid c41 orderdate 20170429 shipdate 20170503 items itemno 347 qty 5 price 1999 itemno 193 qty 2 price 2889 n orderno 1002 custid c13 orderdate 20170501 shipdate 20170503 items itemno 460 qty 95 price 10099 itemno 680 qty 150 price 875 n orderno 1003 custid c31 orderdate 20170615 shipdate 20170616 items itemno 120 qty 2 price 8899 itemno 460 qty 3 price 9999 n orderno 1004 custid c35 orderdate 20170710 shipdate 20170715 items itemno 680 qty 6 price 999 itemno 195 qty 4 price 3500 n orderno 1005 custid c37 orderdate 20170830 items itemno 460 qty 2 price 9998 itemno 347 qty 120 price 2200 itemno 780 qty 1 price 150000 itemno 375 qty 2 price 14998 n orderno 1006 custid c41 orderdate 20170902 shipdate 20170904 items itemno 680 qty 51 price 2598 itemno 120 qty 65 price 8500 itemno 460 qty 120
itemno 460 qty 2 price 9998 itemno 347 qty 120 price 2200 itemno 780 qty 1 price 150000 itemno 375 qty 2 price 14998 n orderno 1006 custid c41 orderdate 20170902 shipdate 20170904 items itemno 680 qty 51 price 2598 itemno 120 qty 65 price 8500 itemno 460 qty 120 price 9998 n orderno 1007 custid c13 orderdate 20170913 shipdate 20170920 items itemno 185 qty 5 price 2199 itemno 680 qty 1 price 2050 n orderno 1008 custid c13 orderdate 20171013 items itemno 460 qty 20 price 9999 n n n demodbcustomersinsertmanycustomersn demodbordersinsertmanyordersn n numcustomers demodbcustomerscountdocumentsn numorders demodborderscountdocumentsn n printfthere are numcustomers customers and numorders orders celltype code executioncount null id 4b977e9f6cc54afb91edfc7eec781c4e metadata scrolled true outputs source the key id attribute is automatically returned unless you explicitly say to remove it n n select name rating from customersn data demodbcustomersfind name1 rating1n printdumpsdata indent2 celltype code executioncount null id 38d4b61959894962b3e6ec1893095d72 metadata scrolled true outputs source now without the id field n n select name rating from customersn data demodbcustomersfind name1 rating1 id0n printdumpsdata indent2 celltype markdown id 1be795017f8b43afa9559e9f95f842f2 metadata source all fields except specific ones returned celltype code executioncount null id 48385e0ffe324abd91a0ef8e732046c3 metadata scrolled true outputs source for every customer
metadata scrolled true outputs source now without the id field n n select name rating from customersn data demodbcustomersfind name1 rating1 id0n printdumpsdata indent2 celltype markdown id 1be795017f8b43afa9559e9f95f842f2 metadata source all fields except specific ones returned celltype code executioncount null id 48385e0ffe324abd91a0ef8e732046c3 metadata scrolled true outputs source for every customer return all fields except id and addressn n data demodbcustomersfind id 0 address 0n printdumpsdata indent2 celltype markdown id d929a63257de4de580496dada9027e6b metadata source equivalent to sql like operator celltype code executioncount null id 84e41240518641f1ba943935e333e255 metadata outputs source select name rating from customers where name like tn n regular expression explanationn match beginning of linen t match literal character t at the beginning of the line in this casen match any single character except newlinen match zero or more occurrences of the previous character the in this casen n data demodbcustomersfindname regex t id 0 name 1 rating1n printdumpsdata indent2 celltype markdown id 95c0ac04ab9d4a34850a16137cd1fc1d metadata source sorting and limiting celltype code executioncount null id e07bf0b910f14fae97f019ff2afda9bf metadata outputs source select name rating from customers order by rating limit 2n n data demodbcustomersfind id 0 name 1 rating1 sortratinglimit2n printdumpsdata indent2 celltype code executioncount null id 08236f133f9447dabd19f931c59039e4 metadata outputs source same as above but
id 95c0ac04ab9d4a34850a16137cd1fc1d metadata source sorting and limiting celltype code executioncount null id e07bf0b910f14fae97f019ff2afda9bf metadata outputs source select name rating from customers order by rating limit 2n n data demodbcustomersfind id 0 name 1 rating1 sortratinglimit2n printdumpsdata indent2 celltype code executioncount null id 08236f133f9447dabd19f931c59039e4 metadata outputs source same as above but sorting in desc ordern n select name rating from customers order by rating desc limit 2n n data demodbcustomersfind id 0 name 1 rating1 sortrating 1limit2n printdumpsdata indent2 celltype code executioncount null id 7ae8e68534d549b3af1e5a0f5ed6186c metadata outputs source providing 2 sort keys n n data demodbcustomersfind id 0 name 1 rating1 sortrating 1 name 1limit2n printdumpsdata indent2 celltype markdown id 143f4349d0cf4812a27cf627f709d8ae metadata source your turn with mflix db celltype markdown id 3e6cbf7978134df0838600b4c133ba0a metadata source question 1 celltype code executioncount 16 id ca28ccf2bff143bea1b62c7f735e4c3b metadata outputs source how many users are there in the mflix database how many moviesn celltype markdown id e520f1c3f9bf46688d847ef605c5bd90 metadata source question 2 celltype code executioncount null id 224682d354484fcd83b0b4938b083b24 metadata outputs source which movies have a rating of tvg only return the title and yearn n celltype markdown id 2a86daefc330493d92d538517d7c4381 metadata source question 3 celltype code executioncount null id 566f45002eb54190abdfbfb9228d5c3f metadata scrolled true outputs source which movies have a
metadata source question 2 celltype code executioncount null id 224682d354484fcd83b0b4938b083b24 metadata outputs source which movies have a rating of tvg only return the title and yearn n celltype markdown id 2a86daefc330493d92d538517d7c4381 metadata source question 3 celltype code executioncount null id 566f45002eb54190abdfbfb9228d5c3f metadata scrolled true outputs source which movies have a runtime of less than 20 minutes only return the title and runtime of each movie n n celltype markdown id 05d0bfebeb464fe0a25754b632f1c34f metadata source question 4 celltype code executioncount null id 86821f3ea95e40fda8bb3afb7078cc6b metadata outputs source how many theaters are in mn or man n celltype markdown id d3c90cac74bc4b699a08b8118bf97bfd metadata source question 5 celltype code executioncount null id 03c20e7f2d7247c5b104083cca731cb5 metadata scrolled true outputs source give the names of all movies that have no comments yet make sure the names are in alphabetical order n n celltype markdown id dd98ae0a0d114d33a5a805f6a7da052e metadata source question 6 celltype code executioncount null id 2c5e0923d2ec4e86a85f57c4ef523b6f metadata scrolled true outputs source return a list of movie titles and all actors from any movie with a title that contains the word four n sort the list by title n metadata kernelspec displayname python 3 ipykernel language python name python3 languageinfo codemirrormode name ipython version 3 fileextension py mimetype textxpython name
outputs source return a list of movie titles and all actors from any movie with a title that contains the word four n sort the list by title n metadata kernelspec displayname python 3 ipykernel language python name python3 languageinfo codemirrormode name ipython version 3 fileextension py mimetype textxpython name python nbconvertexporter python pygmentslexer ipython3 version 3119 nbformat 4 nbformatminor 5
